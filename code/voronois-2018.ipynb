{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagramas de Voronoi – como vota meu vizinho?\n",
    "Esse script gera os [diagramas de Voronoi](https://pt.wikipedia.org/wiki/Diagrama_de_Voronoy) para a matéria sobre a geografia detalhada no voto nas Eleições de 2018. Ele cria o diagrama com base nos votos **presidenciais**. Com os devidos ajustes nas bases de dados, pode gerar polígonos para qualquer outro cargo. O usuário apenas precisa ficar atento para evitar que cargos com muitos candidatos (especialmente deputados estaduais e federais) superem os limites de colunas dos arquivos `.shp`.\n",
    "\n",
    "Note que, nesse caso, estamos usando os dados do arquivo de **boletim de urna** do TSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparação dos dados e pacotes necessários para análise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação de bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisamos de muuuuuitas coisas\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shapely\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "import requests\n",
    "import time\n",
    "import googlemaps\n",
    "import glob\n",
    "import scipy.spatial as spatial\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as path\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import linecache\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm # Progress monitoring\n",
    "import fiona\n",
    "\n",
    "# Define configurações\n",
    "tqdm.pandas()\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparação da chave da API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key do Google Maps\n",
    "gmaps = googlemaps.Client(key='INSIRA SUA CHAVE DE API AQUI') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lê arquivo com lista de locais de votação de 2018 e deixa ele no formato necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {'SGL_UF':'str', 'COD_LOCALIDADE_IBGE':'str', 'LOCALIDADE_LOCAL_VOTACAO':'str', 'ZONA':'str',\n",
    "    'BAIRRO_ZONA_SEDE':'str', 'LATITUDE_ZONA':'float', 'LONGITUDE_ZONA':'float',\n",
    "    'NUM_LOCAL':'str', 'SITUACAO_LOCAL':'str', 'TIPO_LOCAL':'str', 'LOCAL_VOTACAO':'str', \n",
    "    'ENDERECO':'str', 'BAIRRO_LOCAL_VOT':'str', 'CEP':'str', 'LATITUDE_LOCAL':'float', \n",
    "    'LONGITUDE_LOCAL':'float', 'NUM_SECAO':'str', 'SECAO_AGREGADORA':'str', 'SECAO_AGREGADA':'str'\n",
    "    }\n",
    "base = pd.read_csv('../data/locais/local-votacao-08-08-2018.csv', encoding='Latin5', sep=';',decimal=',', dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é tirar a sujeira de parte desse banco de dados para facilitar a geolocalização. Depois de alguma manual, percebemos alguns padrões que podem ser evitados. Os locais de Salvador, por exemplo, são identificados como \"Zona Urbana\" e \"Zona Rural\". Isso é desnecessário e complica o trabalho do Google na hora de localizar. Vamos remover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base['ENDERECO'] = base.ENDERECO.str.replace(\" - ZONA URBANA\",\"\")\n",
    "base['ENDERECO'] = base.ENDERECO.str.replace(\" - ZONA RURAL\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um campo único para evitar consultas duplas\n",
    "base['local_unico'] = base.LOCAL_VOTACAO + ', ' + base.ENDERECO + ', ' + base.LOCALIDADE_LOCAL_VOTACAO + ', ' + base.SGL_UF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um id único com base na query - permite juntar cada par de coordenadas a uma localidade única\n",
    "base['id_unico'] = base.groupby('local_unico').grouper.group_info[0].astype(str)\n",
    "base.to_csv(\"../data/locais/base-por-secao.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cria uma cópia do campo, sem duplicatas - útil para geolocalizar cada endereço apenas uma vez\n",
    "locais = base.drop_duplicates(subset='id_unico').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforma o campo da variável locais em string.\n",
    "# Por algum motivo, ao fazer o subset, o dado voltou a ser tratado com int.\n",
    "locais.id_unico = locais.id_unico.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adiciona um campo para o país\n",
    "locais['COUNTRY'] = 'BRASIL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preenche campos com valores temporários, mas que serão necessários para iniciar o processo de geolocalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_precision(row):\n",
    "\n",
    "    # Função simples para determinar se a escola já tem um valor de lat/long\n",
    "    # Se tiver, isso significa que ela já foi geocodificada pelo TSE\n",
    "    # Assim, definimos uma categoria de precisão: \"TSE\"\n",
    "    # As restantes serão classificadas, temporariamente, como None\n",
    "    # E terão esse valor preenchido futuramente\n",
    "\n",
    "    \n",
    "    if np.isnan(row.LATITUDE_LOCAL):\n",
    "        precision = 'NO_VALUE'\n",
    "        fetched_address = 'NO_VALUE'\n",
    "        lon = np.nan\n",
    "        lat = np.nan\n",
    "        \n",
    "    else:\n",
    "        precision = 'TSE'\n",
    "        fetched_address = 'TSE'\n",
    "        lon = row.LONGITUDE_LOCAL\n",
    "        lat = row.LATITUDE_LOCAL\n",
    "        \n",
    "    return pd.Series({\n",
    "            \"precision\":precision,\n",
    "            \"fetched_address\":fetched_address,\n",
    "            \"lat\":lat,\n",
    "            \"lon\":lon,\n",
    "            })\n",
    "\n",
    "locais[[\"fetched_address\", \"lat\", \"lon\", \"precision\"]] = locais.apply(get_precision, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleta campos que se tornaram redundantes depois dessa preparação. Também reordena as colunas de modo mais intuitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locais = locais[['id_unico', 'local_unico', 'lat', 'lon','fetched_address','precision',\n",
    "                 'LOCAL_VOTACAO', 'ENDERECO', 'CEP',\n",
    "                'COD_LOCALIDADE_IBGE', 'LOCALIDADE_LOCAL_VOTACAO', 'SGL_UF', \n",
    "                'ZONA', 'BAIRRO_ZONA_SEDE', 'LATITUDE_ZONA', 'LONGITUDE_ZONA',\n",
    "                'NUM_SECAO', 'NUM_LOCAL','BAIRRO_LOCAL_VOT', 'COUNTRY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é a estrutura de dados com a qual iremos trabalhar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Geolocalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a API do Google, geoferenciamos os locais de votação das Eleições de 2018 nas oito principais capitais do Brasil.\n",
    "\n",
    "**Input:** um arquivo produzido pelo TSE com a localização das **sessões eleitorais**. Alguns pontos já tem latitude e longitude, mas nem todos. Eles estão organizados por sessão – ou seja, por urna. Ele foi tratado previamente no Open Refine para padronizar alguns campos, como nomes de rua com erros de digitação.\n",
    "    \n",
    "**Output:** um arquivo com **locais de votação** únicos, com latitude e longitude para todos os itens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso, vamos definir uma função para geolocalizar os elementos necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Função para localizar os colégios eleitorais que não foram georeferenciados pelo TSE\n",
    "def get_coordinates(row, query_params, exception_list):\n",
    "    ### Essa função deve ser rodada pelo método df.apply() do Pandas\n",
    "    ### Ela recebe um parâmetro 'query', que se refere ao tipo de busca que queremos realizar.   \n",
    "    ### Ele deve ser passado na forma de uma lista com a combinação das colunas que vão ser passadas para a API, em ordem.\n",
    "    ### Considere, por exemplo, query = ['ENDERECO', 'LOCALIDADE_LOCAL_VOTACAO', 'SGL_UF'].\n",
    "    ### Isso faria uma busca por \"Rua A, Cidade B, DF\" no Google Maps.\n",
    "    ### Note que ele checa se a precisão da linha é superior a precisão encontrada na request antes de substituir.\n",
    "    ### exception_list é necessariamente uma lista vazia, que serve para guardar as exceções item a item.\n",
    "    \n",
    "    ##### INÍCIO DAS FUNCIONALIDADES ####\n",
    "    \n",
    "    # Aqui, atribuímos um valor numérico para cada uma das possíveis classificações de precisão.\n",
    "    # Assim, podemos checar se a precisão que recuperamos ao enviar a requisição é melhor que a atual.\n",
    "    # Se for assim, atualizamos. Caso contrário, mantemos o anterior.\n",
    "    precision_order = {\n",
    "                       'TSE':1,\n",
    "                       'ROOFTOP':2,\n",
    "                       'RANGE_INTERPOLATED':3,\n",
    "                       'GEOMETRIC_CENTER':4,\n",
    "                       'APPROXIMATE':5,\n",
    "                       'NO_VALUE':6,\n",
    "                       'ERROR':7,\n",
    "                      } \n",
    "    \n",
    "    # Agora, precisamos construir a query customizada que foi passada como parâmetro\n",
    "    query = [str(row[item]).strip() for item in query_params]\n",
    "    query = ', '.join(query)\n",
    "\n",
    "    # Com esses parâmetros definidos, já podemos começar a georeferenciar    \n",
    "    try:\n",
    "        # Primeiro, verificamos se o item que queremos avaliar já tem uma precisão adequada.\n",
    "        # Ou seja, se tem uma precisão de nível 1 ou 2 ('TSE' e 'ROOFTOP').\n",
    "        # Caso positivo, podemos ignorar e manter os valores antigos.\n",
    "        # Caso contrário, segue para o referenciamento.\n",
    "        if precision_order[row.precision] < 3:\n",
    "            lat = row.lat\n",
    "            lon = row.lon\n",
    "            precision = row.precision\n",
    "            fetched_address = row.fetched_address\n",
    "        \n",
    "        # Aqui, já sabemos que o item não alcança nosso nível mínimo de precisão.\n",
    "        else:\n",
    "            \n",
    "            # Envia a requisição para o Google.\n",
    "            geocode_result = gmaps.geocode(query)\n",
    "\n",
    "            # Descobre a precisão do resultado obtido.\n",
    "            precision = geocode_result[0]['geometry']['location_type']\n",
    "\n",
    "            # Se a precisão for maior do que a observada, podemos substituir.\n",
    "            # Essa checagem é importante para evitar que um resultado com precisão 'RANGE_INTERPOLATED'\n",
    "            # seja trocado por um 'ERROR', por exemplo.\n",
    "            if precision_order[precision] < precision_order[row.precision]:\n",
    "\n",
    "                lat = geocode_result[0]['geometry']['location']['lat']\n",
    "                lon = geocode_result[0]['geometry']['location']['lng']\n",
    "                fetched_address = geocode_result[0]['formatted_address']\n",
    "\n",
    "            # Se não, mantemos o valor antigo.\n",
    "            else:\n",
    "                lat = row.lat\n",
    "                lon = row.lon\n",
    "                precision = row.precision\n",
    "                fetched_address = row.fetched_address\n",
    "\n",
    "    # Caso o georeferenciamento falhe, mantemos o valor que existia anteriormente.\n",
    "    except:\n",
    "        print('ERROR!')\n",
    "        print()\n",
    "        exception_list.append(row.local_unico)\n",
    "        lat = row.lat\n",
    "        lon = row.lon\n",
    "        fetched_address = row.fetched_address\n",
    "        precision = row.precision\n",
    "            \n",
    "    return pd.Series({\n",
    "        'lon':lon,\n",
    "        'lat':lat,\n",
    "        'precision':precision,\n",
    "        'fetched_address':fetched_address,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos rodar a função cidade por cidade, para evitar que um desconexão cause a perda de todo o progresso. Os arquivos são salvos sempre que o geocoding de uma cidade acabar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def geocode_each_city(cities, df, query_params, try_no, exception_list):\n",
    "    \n",
    "    ### Essa função roda o código da função get_coordinates(row, query_params, exception_list)\n",
    "    ### A diferença é que faz isso aos poucos, cidade por cidade, salvando um arquivo sempre que termina.\n",
    "    ### Os argumentos são os seguintes:\n",
    "    ### cities: lista de cidades para geolocalizar\n",
    "    ### df: o dataframe com os locais de votação\n",
    "    ### query_paramd: a lista de campos que compõe a request que deve ser enviada para o goOGLE\n",
    "    ### try_no: uma string que vai servir como identificador dos arquivos. Ela, idealmente,\n",
    "    ### deve ser um número. Assim, conseguimos controlar versões depois de rodar o código diversas vezes\n",
    "    ### com mudanças na variável query_param.\n",
    "    ### exception_list: a mesma lista de exceções da função get_coordinates(...)\n",
    "    \n",
    "    for city in cities:\n",
    "\n",
    "        # Cria um df filtrado coma  cidade que queremos\n",
    "        temp = df[df.COD_LOCALIDADE_IBGE==city]\n",
    "        \n",
    "        # Salva os dados para o df filtrado\n",
    "        data = temp.apply(get_coordinates, args=(query_params,exception_list), axis=1)\n",
    "        \n",
    "        # Passa os dados para o df original\n",
    "        temp[data.columns] = data\n",
    "        \n",
    "        # Salva uma primeira versão para não precisar repetir todo o processo\n",
    "        temp.to_csv('../data/geocode/geocode-' + str(try_no) + '-' + 'try' + str(city) + '.csv', index=False)\n",
    "        \n",
    "        # Também salva um log de excessões\n",
    "        newfile = '../data/error-logs/errorlog'+ str(try_no) + '-' + 'try' + str(city) + '.txt'\n",
    "        exception_dump = '\\n'.join(exception_list)\n",
    "        \n",
    "        with open(newfile, 'w') as outfile:\n",
    "            outfile.write(exception_dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como temos vários arquivos salvos depois do processo de geolocalização, precisamos concatená-los em um único arquivo depois de terminar o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenate_files(try_no):\n",
    "    \n",
    "    ### Essa função lê os arquivos que foram salvos ao executar geocode_each_city(cities, df, query_params, try_no).\n",
    "    ### Depois, concatena todos os dataframes e retorna uma única variável.\n",
    "    ### O único argumento é try_no – o mesmo da função geocode.\n",
    "    ### Sua função é identificar quais arquivos devem ser concantenados.\n",
    "    \n",
    "    files = glob.glob(\"../data/geocode/geocode-\" + str(try_no) + \"*.csv\")\n",
    "    dfs = []\n",
    "    dtype = {\n",
    "        'SGL_UF':'str', 'COD_LOCALIDADE_IBGE':'str', 'LOCALIDADE_LOCAL_VOTACAO':'str', 'ZONA':'str',\n",
    "        'BAIRRO_ZONA_SEDE':'str', 'LATITUDE_ZONA':'float', 'LONGITUDE_ZONA':'float',\n",
    "        'NUM_LOCAL':'str', 'SITUACAO_LOCAL':'str', 'TIPO_LOCAL':'str', 'LOCAL_VOTACAO':'str', \n",
    "        'ENDERECO':'str', 'BAIRRO_LOCAL_VOT':'str', 'CEP':'str', 'LATITUDE_LOCAL':'float', \n",
    "        'LONGITUDE_LOCAL':'float', 'NUM_SECAO':'str', 'SECAO_AGREGADORA':'str', 'SECAO_AGREGADA':'str',\n",
    "        'id_unico':'str',\n",
    "       }\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file, dtype=dtype)\n",
    "        dfs.append(df)\n",
    "        \n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir uma função que executa essas ações em ordem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geocode(cities, df, query_params, try_no, exception_list):\n",
    "    geocode_each_city(cities, df, query_params, try_no, exception_list)\n",
    "    df = concatenate_files(try_no).reset_index(drop=True) # reset_index para manter índices únicos após a concatenação\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfim, rodamos a função, com uma sequência de combinações possíveis para as requisições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_queries = [\n",
    "    \n",
    "    ['LOCAL_VOTACAO', 'ENDERECO', 'LOCALIDADE_LOCAL_VOTACAO', 'SGL_UF', 'COUNTRY'],\n",
    "    ['LOCAL_VOTACAO', 'CEP', 'LOCALIDADE_LOCAL_VOTACAO', 'SGL_UF', 'COUNTRY'],\n",
    "    ['LOCAL_VOTACAO', 'LOCALIDADE_LOCAL_VOTACAO', 'SGL_UF', 'COUNTRY'],\n",
    "    ['ENDERECO', 'BAIRRO_LOCAL_VOT', 'LOCALIDADE_LOCAL_VOTACAO', 'CEP', 'SGL_UF', 'COUNTRY'],\n",
    "    ['ENDERECO', 'CEP', 'LOCALIDADE_LOCAL_VOTACAO', 'SGL_UF', 'COUNTRY'],\n",
    "    ['ENDERECO', 'LOCALIDADE_LOCAL_VOTACAO', 'SGL_UF', 'COUNTRY'],\n",
    "    \n",
    "]\n",
    "\n",
    "def run(all_queries, df, str_id):\n",
    "    \n",
    "    # Executa tentativas de geocodificação com\n",
    "    # vários parâmetros possíveis.\n",
    "    # str_id é um identificador do arquivo final\n",
    "    \n",
    "    cities = df.COD_LOCALIDADE_IBGE.unique()\n",
    "    all_tries = range(1, len(all_queries) + 1)\n",
    "    \n",
    "    for query_params, try_no in zip(all_queries, all_tries):\n",
    "        exception_list = []\n",
    "        df = geocode(cities, df, query_params, try_no, exception_list)\n",
    "        \n",
    "    # A função salva apenas o último output, que é o estado dos dados após passar por todas as queries definidas\n",
    "    df.to_csv(\"../data/geocode/geocode-\" + str_id + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roda com um timestamp para identificar quando foi salvo\n",
    "now = str(datetime.now())[:-7]\n",
    "now = now.replace(' ','_')\n",
    "now = now.replace(\":\",\"-\")\n",
    "run(all_queries, locais, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-lê o arquivo para reiniciar trabalho\n",
    "dtype = {\n",
    "    'SGL_UF':'str', 'COD_LOCALIDADE_IBGE':'str', 'LOCALIDADE_LOCAL_VOTACAO':'str', 'ZONA':'str',\n",
    "    'BAIRRO_ZONA_SEDE':'str', 'LATITUDE_ZONA':'float', 'LONGITUDE_ZONA':'float',\n",
    "    'NUM_LOCAL':'str', 'SITUACAO_LOCAL':'str', 'TIPO_LOCAL':'str', 'LOCAL_VOTACAO':'str', \n",
    "    'ENDERECO':'str', 'BAIRRO_LOCAL_VOT':'str', 'CEP':'str', 'LATITUDE_LOCAL':'float', \n",
    "    'LONGITUDE_LOCAL':'float', 'NUM_SECAO':'str', 'SECAO_AGREGADORA':'str', 'SECAO_AGREGADA':'str',\n",
    "    'id_unico':'str',\n",
    "   }\n",
    "locais = pd.read_csv(\"../data/geocode/geocode-\" + now + \"csv\", dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROOFTOP               0.418821\n",
       "TSE                   0.236085\n",
       "GEOMETRIC_CENTER      0.212389\n",
       "APPROXIMATE           0.103929\n",
       "RANGE_INTERPOLATED    0.028724\n",
       "NO_VALUE              0.000053\n",
       "Name: precision, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O que temos, e em que nível de precisão?\n",
    "locais.precision.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para garantir um mínimo de precisão a partir de agora, vamos trabalhar apenas com dados classificados como `ROOFTOP` ou `TSE`. Um teste de amostra indicou que 94% das entradas `ROOFTOP` estão de fato corretas. As entradas `TSE` foram geolocalizadas manualmente pelo tribunal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locais = locais[locais.precision.isin(['ROOFTOP', 'TSE'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É provável que algumas cidades pequenas fiquem sem nenhum local de votação localizado. Esses casos serão tratados posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Verificar integridade dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, temos de fazer uma bateria de testes pare verificar se os dados estão corretos – se há entradas duplicadas e se os pontos estão no interior dos municípios onde deveriam estar, por exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "De início, vamos verificar se há duplicatas. O Pandas consegue checar apenas strings. Assim, precisamos criar uma nova coluna, que combina a latitude e longitude para criar um elemento checável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Primeiro, aplicamos uma redução de precisão nos pontos encontrados. Queremos que eles sejam precisos até a quarta casa decimal.\n",
    "# O Google, geralmente, retorna precisão até a sexta\n",
    "locais[\"lat\"] = locais.lat.round(4)\n",
    "locais[\"lon\"] = locais.lon.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lat_lon_str(row):\n",
    "    \n",
    "    # Formata a latitude e longitude sem perder precisão na conversão\n",
    "    \n",
    "    lat = \"{:.4f}\".format(row.lat)\n",
    "    lon = \"{:.4f}\".format(row.lon)\n",
    "    \n",
    "    lat_lon_str = lat + ', ' + lon\n",
    "    \n",
    "    return pd.Series({'lat_lon_str':lat_lon_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locais['lat_lon_str'] = locais.apply(get_lat_lon_str, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    52372\n",
       "True      9622\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conta as entradas duplicadas.\n",
    "# Note o parêametro keep=False, que conta todos os itens e não apenas a primeira das repetições\n",
    "locais.duplicated(subset='lat_lon_str', keep=False).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há entradas com coordenadas geográficas duplicadas. Em alguns casos, isso pode ser correto: dois locais de votação que ocupam prédios diferentes de uma universidade, por exemplo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notícia boa é que, como todas elas tem precisão `ROOFTOP` ou `TSE`, são **muito provalmente** duplicatas legítimas, possivelmente locais de votação categorizados de maneira distinta, mas que ocupam o mesmo prédio. Não temos condições de checar todas estas manualmente, então vamos assumir que estão corretas - os votos destas sessões serão agregados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra checagem necessária é ver se todos os pontos que geolocalizamos se encontram nos limites de suas cidades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, precisamos selecionar os mapas das cidades, que pegamos do IBGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_maps = gpd.read_file(\"../data/geo/municipios-ibge/brasil-municipios/brasil-municipios.shp\")\n",
    "city_maps[\"NM_MUNICIP\"] = city_maps.NM_MUNICIP.astype(str)\n",
    "city_maps[\"CD_GEOCMU\"] = city_maps.CD_GEOCMU.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o restante do trabalho, precisamos ter certeza que tanto os mapas municipais quanto os pontos que geolocalizamos tem o mesmo CRS – ou seja, o mesmo sitema de coordenadas geográficas. Vamos em diante:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, transformamos o objeto `locais` em um geodataframe, usando o CRS específico para o Google Maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geometry = [ Point( ( row.lon, row.lat ) ) for index, row in locais.iterrows() ]\n",
    "locais = gpd.GeoDataFrame( locais, geometry = geometry )\n",
    "locais.crs = {'init': 'EPSG:4326'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, transformamos os dados do IBGE para que fiquem nesse mesmo crs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_maps = city_maps.to_crs(locais.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As próximas linhas de código determinam se os pontos são válidos, mantendo apenas os que estão dentro dos respectivos municípios. **Possivelmente, vamos perder alguns pontos.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_points(locais, city_maps):\n",
    "    \n",
    "# A função remove pontos que, por ventura, tenham sido localizados fora dos limites de suas respectivas cidades.\n",
    "\n",
    "    to_concat = []\n",
    "    count = 0\n",
    "    \n",
    "    for city_code in tqdm( city_maps.CD_GEOCMU.unique() ):\n",
    "        flag = ''\n",
    "        this_city = city_maps[city_maps.CD_GEOCMU == city_code].reset_index(drop=True)\n",
    "        points = locais[locais.COD_LOCALIDADE_IBGE==city_code].reset_index(drop=True)\n",
    "\n",
    "        city_name = this_city.loc[0, 'NM_MUNICIP']\n",
    "        locais_validos = gpd.sjoin(points, this_city, how='inner', op='intersects')\n",
    "        to_concat.append(locais_validos)\n",
    "        \n",
    "    locais_validos = pd.concat(to_concat).reset_index(drop=True)\n",
    "    return locais_validos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos derrubar todos os pontos que se encontram fora dos limites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locais = validate_points(locais, city_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locais.to_csv(\"../data/locais/locais-validos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Ligar os votos ao local de votação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos trabalhar com a variável `df` – ela é a lista **pura** dos locais de votação, sem os filtros aplicados previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há um problema: os códigos do TSE e do IBGE não são os mesmos, o que dificulta fazer o merge. Vamos usar um outro arquivo para fazer essa correspondência na tabela `locais`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correspondencia = pd.read_csv(\"../data/votos/correspondencia-tse-ibge.csv\", dtype={\n",
    "    'GEOCOD_IBGE':'str',\n",
    "    'COD_TSE':'str',\n",
    "})\n",
    "\n",
    "correspondencia[\"COD_TSE\"] = correspondencia.COD_TSE.str.zfill(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, vamos criar um identificador único real para cada uma das seções. Isso é necessário porque o número identificador do TSE **não identifica a sessão de maneira única**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = base.merge(correspondencia[['GEOCOD_IBGE', 'COD_TSE']], \n",
    "                      left_on='COD_LOCALIDADE_IBGE', right_on='GEOCOD_IBGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, lemos o arquivo com os boletins de urna do TSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'DT_GERACAO':'str', \n",
    "    'HH_GERACAO':'str',\n",
    "    'ANO_ELEICAO':'str', \n",
    "    'CD_PLEITO':'str', \n",
    "    'DT_PLEITO':'str', \n",
    "    'NUM_TURNO':'str', \n",
    "    'CD_ELEICAO':'str', \n",
    "    'DS_ELEICAO':'str', \n",
    "    'DT_ELEICAO':'str', \n",
    "    'SG_UF':'str', \n",
    "    'COD_TSE':'str', \n",
    "    'NM_MUNICIPIO':'str', \n",
    "    'ZONA':'str',\n",
    "    'NUM_SECAO':'str', \n",
    "    'NR_LOCAL_VOTACAO':'str', \n",
    "    'CODIGO_CARGO':'str',\n",
    "    'DS_CARGO_PERGUNTA':'str',\n",
    "    'NR_PARTIDO':'str', \n",
    "    'SG_PARTIDO':'str', \n",
    "    'NM_PARTIDO':'str', \n",
    "    'QT_APTOS':'str',\n",
    "    'QT_COMPARECIMENTO':'str',\n",
    "    'QT_ABSTENCOES':'str',\n",
    "    'CD_TIPO_URNA':'str', \n",
    "    'DS_TIPO_URNA':'str', \n",
    "    'CD_TIPO_VOTAVEL':'str', \n",
    "    'DS_TIPO_VOTAVEL':'str', \n",
    "    'NUM_VOTAVEL':'str',\n",
    "    'NM_VOTAVEL':'str',\n",
    "    'QTDE_VOTOS':'int',\n",
    "    'NR_URNA_EFETIVADA':'str',\n",
    "    'CD_CARGA_1_URNA_EFETIVADA':'str',\n",
    "    'CD_CARGA_2_URNA_EFETIVADA':'str',\n",
    "    'CD_FLASHCARD_URNA_EFETIVADA':'str',\n",
    "    'DT_CARGA_URNA_EFETIVADA':'str',\n",
    "    'DS_CARGO_PERGUNTA_SECAO':'str',\n",
    "    'DS_AGREGADAS':'str',\n",
    "    'DT_ABERTURA':'str',\n",
    "    'DT_ENCERRAMENTO':'str',\n",
    "    'QT_ELEITORES_BIOMETRIA_NH':'str',\n",
    "    'NR_JUNTA_APURADORA':'str',\n",
    "    'NR_TURMA_APURADORA':'str',\n",
    " \n",
    "}\n",
    "\n",
    "usecols = [\n",
    "    'ANO_ELEICAO', 'CD_PLEITO', 'NUM_TURNO',\n",
    "    'SG_UF', 'COD_TSE', 'NM_MUNICIPIO', 'ZONA', 'NUM_SECAO', 'NR_LOCAL_VOTACAO',\n",
    "    'CODIGO_CARGO', 'DS_CARGO_PERGUNTA', 'NR_PARTIDO', 'SG_PARTIDO', 'QT_APTOS', 'QT_COMPARECIMENTO',\n",
    "    'QT_ABSTENCOES', 'CD_TIPO_VOTAVEL', 'DS_TIPO_VOTAVEL', 'NUM_VOTAVEL', 'QTDE_VOTOS',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 48.9 s, total: 3min 4s\n",
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "votos_por_secao = pd.read_csv(\"../data/votos/boletim-urna/boletins-concatenados-2-turno.csv\",\n",
    "                              encoding='Latin1', sep=',', header=0, names=usecols, \n",
    "                              dtype={col:'str' for col in usecols if 'QT' not in col}, \n",
    "                              usecols=usecols\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_right_votes(df, cod_cargo, num_turno):\n",
    "    \n",
    "    # Seleciona os votos para a eleição - presidencial em primeiro turno, por exemplo\n",
    "    \n",
    "    votos_por_secao = df[(df.CODIGO_CARGO==str(cod_cargo)) & (df.NUM_TURNO==str(num_turno))]\n",
    "    \n",
    "    return votos_por_secao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_unique_id(df):\n",
    "    \n",
    "    # Cria um id único para cada sessão eleitoral\n",
    "    \n",
    "    df['ID_SECAO'] = df['COD_TSE'].astype(str) + '-' + df['ZONA'].astype(str) + '-' + df['NUM_SECAO'].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_votes(df, votos_por_secao):\n",
    "    \n",
    "    # Reúne os votos da planilha de resultados com os locais de votação pré-catalogados\n",
    "    \n",
    "    votes = df.merge(votos_por_secao, on='ID_SECAO', how='inner', suffixes=[\"\",\"__y\"])\n",
    "    votes = votes[[col for col in votes.columns if \"__y\" not in col]]\n",
    "    \n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_places(df, locais):\n",
    "    \n",
    "    # Reúne os locais de votação geolocalizados com os dados de votos.\n",
    "    # Algumas seções devem se perder, já que nem todos os locais do país foram geolocalizados.\n",
    "    \n",
    "    votes = df.merge(locais, on='id_unico', how='inner', suffixes=[\"\",\"__y\"])\n",
    "    votes = votes[[col for col in votes.columns if \"__y\" not in col]]\n",
    "    \n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pivot_by_candidates(df, field):\n",
    "    \n",
    "    # Pega os dados que estão em formato long, com os dados de voto de cada candidato em uma linha\n",
    "    # e transforma para o formato wide, com os votos de cada candidato em dado local como uma coluna do banco de dados\n",
    "    \n",
    "    temp_df = df.copy()\n",
    "    \n",
    "    columns = df.NUM_VOTAVEL.unique().tolist()\n",
    "    votes = df.pivot_table(values='QTDE_VOTOS', index=field, columns='NUM_VOTAVEL', aggfunc='sum', fill_value=0)\n",
    "    votes = votes.reset_index()\n",
    "    # Duplicadas já contidas na pivot_table\n",
    "    temp_df = temp_df.drop_duplicates(subset=field)\n",
    "    \n",
    "    votes = temp_df.merge(votes, on=field, how='inner')\n",
    "    \n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def groubpy_coords(df, cand_nos):\n",
    "    \n",
    "    # Agrupa os votos por coordenada geográfica, de modo a concentrar\n",
    "    # locais de votação com ids distintos, mas que ocupam o mesmo espaço,\n",
    "    # em um único ponto. É o caso de locais como \"PUC - Bloco A\" e \"PUC - Bloco B\"\n",
    "    \n",
    "    votes = df.copy()\n",
    "    grouped_votes = df.groupby('lat_lon_str')[cand_nos].sum().reset_index()\n",
    "    votes = votes.merge(grouped_votes, on='lat_lon_str', how='inner', suffixes=(\"__x\",\"\")) # Mantém os campos do groupby, que é o df da direita neste join, sem sufixo\n",
    "    votes = votes.drop_duplicates(subset='lat_lon_str') # Remove duplicatas que se tornaram redundantes com o merge\n",
    "    votes = votes[[col for col in votes.columns if '__x' not in col]]\n",
    "    \n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_votes(df, vote_data, locais, cod_cargo, num_turno):\n",
    "    \n",
    "    # Função que encapsula e roda as anteriores, com apenas um output.\n",
    "    \n",
    "    # Faz cópias para trabalhar no espectro local\n",
    "    temp_df = df.copy()\n",
    "    temp_vote_data = vote_data.copy()\n",
    "    temp_locais = locais.copy()\n",
    "    \n",
    "    # Seleciona votos corretos\n",
    "    votos_selecionados = select_right_votes(vote_data, cod_cargo, num_turno)\n",
    "    \n",
    "    # Cria códigos únicos para ligar os votos com suas respectivas sessões\n",
    "    temp_df = make_unique_id(temp_df)\n",
    "    votos_por_secao = make_unique_id(votos_selecionados)\n",
    "    \n",
    "    # Reune votes e seções\n",
    "    votes = join_votes(temp_df, votos_selecionados)\n",
    "    \n",
    "    # Reúne as sessões e seus respectivos locais\n",
    "    votes = join_places(votes, temp_locais)\n",
    "    \n",
    "    # Cria uma pivot table para colocar os votos na mesma linha\n",
    "    votes = pivot_by_candidates(votes, \"id_unico\")\n",
    "    \n",
    "    # Agrupa por local\n",
    "    votes = groubpy_coords(votes, votos_selecionados.NUM_VOTAVEL.unique())\n",
    "    \n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = run_votes(base, votos_por_secao, locais, \"1\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lidar com cidades que têm número insuficientes de locais\n",
    "Para gerar os voronois, precisamos que cada cidade tenha ao menos quatro pontos válidos. Vamos remover os municípios que não atingem esse critério. Seus votos serão agrupados em um único local de votação, criado artificialmente no centróide da cidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_incompletes(df):\n",
    "    '''\n",
    "    REMOVE TODOS OS LOCAIS DE CIDADES QUE NAO TENHAM AO MENOS QUATRO LOCAIS\n",
    "    '''\n",
    "    \n",
    "    # Usa groupby filter para remover os dados das cidades que não desejamos\n",
    "    votes = df.groupby('COD_TSE').filter(lambda x: x['GEOCOD_IBGE'].count() >= 4)\n",
    "    return votes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_centroids(df, city_maps):\n",
    "    '''\n",
    "    ADICIONA OS CENTROIDES DE CADA UMA DAS CIDADES QUE FALTARAM\n",
    "    '''\n",
    "    new_df = pd.DataFrame()\n",
    "    city_list = [city for city in city_maps.CDO_TSE if city not in df.COD_TSE.unique()]\n",
    "    \n",
    "    count = 0\n",
    "    for city in city_list:\n",
    "    # 1. Para cada uma das cidades ausentes no dataframe de votos, criar uma linha com o dados geográficos manualmente e dar append em um novo dataframe\n",
    "    # Note que não vamos adicionar agora ao dataframe original, mas sim criar um novo que passará por um merge posteriormente.\n",
    "        count += 1\n",
    "        city_data = city_maps[city_maps.CDO_TSE==city].reset_index()\n",
    "        city_data = city_data.loc[0]\n",
    "        row = {\n",
    "                 'ANO_ELEICAO': None,\n",
    "                 'BAIRRO_LOCAL_VOT': None,\n",
    "                 'BAIRRO_ZONA_SEDE': None,\n",
    "                 'CDO_TSE': city_data.CDO_TSE,\n",
    "                 'CD_GEOCMU': city_data.CD_GEOCMU,\n",
    "                 'CEP': None,\n",
    "                 'CODIGO_CARGO': None,\n",
    "                 'COD_LOCALIDADE_IBGE': city_data.CD_GEOCMU,\n",
    "                 'COD_TSE': city_data.CDO_TSE,\n",
    "                 'COUNTRY': \"Brasil\",\n",
    "                 'DATA_GERACAO': None,\n",
    "                 'DESCRICAO_CARGO': None,\n",
    "                 'DESCRICAO_ELEICAO': None,\n",
    "                 'ENDERECO': \"Centróide de \" + city_data.NM_MUNICIP,\n",
    "                 'GEOCOD_IBGE': city_data.CD_GEOCMU,\n",
    "                 'HORA_GERACAO': None,\n",
    "                 'ID_SECAO': 'FAKE' + str(count).zfill(4),\n",
    "                 'LATITUDE_LOCAL': None,\n",
    "                 'LATITUDE_ZONA': None,\n",
    "                 'LOCALIDADE_LOCAL_VOTACAO': city_data.NM_MUNICIP,\n",
    "                 'LOCAL_VOTACAO': \"Centróide de \" + city_data.NM_MUNICIP,\n",
    "                 'LONGITUDE_LOCAL': None,\n",
    "                 'LONGITUDE_ZONA': None,\n",
    "                 'NM_MUNICIP': city_data.NM_MUNICIP,\n",
    "                 'NOME_MUNICIPIO': city_data.NM_MUNICIP,\n",
    "                 'NUM_LOCAL': None,\n",
    "                 'NUM_SECAO': None,\n",
    "                 'NUM_TURNO': None,\n",
    "                 'NUM_VOTAVEL': None,\n",
    "                 'QTDE_VOTOS': None,\n",
    "                 'SECAO_AGREGADA': None,\n",
    "                 'SECAO_AGREGADORA': None,\n",
    "                 'SGL_UF': city_data.UF,\n",
    "                 'SIGLA_UE': city_data.UF,\n",
    "                 'SIGLA_UF': city_data.UF,\n",
    "                 'SITUACAO_LOCAL': None,\n",
    "                 'TIPO_LOCAL': None,\n",
    "                 'UF': city_data.UF,\n",
    "                 'ZONA': 'FAKE' + str(count).zfill(4),\n",
    "                 'fetched_address': \"Centróide de \" + city_data.NM_MUNICIP,\n",
    "                 'geometry': city_data.geometry.centroid,\n",
    "                 'id_unico': 'FAKE' + str(count).zfill(4),\n",
    "                 'index_right': None,\n",
    "                 'lat': round(city_data.geometry.centroid.coords[0][1], 4),\n",
    "                 'lat_lon_str': None,\n",
    "                 'local_unico': None,\n",
    "                 'lon': round(city_data.geometry.centroid.coords[0][0], 4),\n",
    "                 'precision': \"CITY_CENTROID\"\n",
    "        }        \n",
    "        new_df = new_df.append(row, ignore_index=True)\n",
    "    \n",
    "    new_df['lat_lon_str'] = new_df.apply(get_lat_lon_str, axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_centroid_votes(df, vote_data, cod_cargo, num_turno, city_maps):\n",
    "    '''\n",
    "    PEGA OS VOTOS DA CIDADE TODA, QUANDO ELA NÃO ESTÁ CONTEMPLADA NOS LOCAIS DE VOTAÇÃO\n",
    "    '''\n",
    "    centroids = df.copy()\n",
    "    vote_data_temp = vote_data.copy()\n",
    "    city_list = [city for city in city_maps.CDO_TSE if city in df.COD_TSE.unique()]\n",
    "    \n",
    "    # Seleciona votos corretos\n",
    "    vote_data_temp = select_right_votes(vote_data_temp, cod_cargo, num_turno)\n",
    "    vote_data_temp = vote_data_temp[vote_data_temp.COD_TSE.isin(city_list)]\n",
    "    \n",
    "    # Agrupa dados dos votos de forma correta\n",
    "    vote_data_temp = pivot_by_candidates(vote_data_temp, \"COD_TSE\")\n",
    "    \n",
    "    # Faz o merge\n",
    "    centroids = centroids.merge(vote_data_temp, on='COD_TSE', how='inner', suffixes=(\"\",\"__y\")) # Note que aqui, quando não estamos trabalhando com o país todo, cidades sem votos registrados podem desaparecer.\n",
    "    centroids = centroids[[col for col in centroids.columns if \"__y\" not in col]]\n",
    "    \n",
    "    # Derruba duplicatas\n",
    "    centroids = centroids.drop_duplicates(subset='COD_TSE')\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenate(df, new_df):\n",
    "    \n",
    "    # Encapsula função built-in apenas por conveniência\n",
    "    \n",
    "    return pd.concat([df, new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_cols(df):\n",
    "    \n",
    "    # Derruba colunas desnecessárioas\n",
    "    \n",
    "    to_drop = [\n",
    "        'ANO_ELEICAO', 'BAIRRO_LOCAL_VOT', 'BAIRRO_ZONA_SEDE', 'CDO_TSE',\n",
    "       'CD_GEOCMU', 'CEP', 'CODIGO_CARGO', 'COD_LOCALIDADE_IBGE',\n",
    "       'COUNTRY', 'DATA_GERACAO', 'DESCRICAO_CARGO', 'DESCRICAO_ELEICAO',\n",
    "       'ENDERECO', 'HORA_GERACAO', 'ID_SECAO', 'LATITUDE_LOCAL',\n",
    "       'LATITUDE_ZONA', 'LOCALIDADE_LOCAL_VOTACAO', 'LOCAL_VOTACAO',\n",
    "       'LONGITUDE_LOCAL', 'LONGITUDE_ZONA', 'NOME_MUNICIPIO',\n",
    "       'NUM_LOCAL', 'NUM_SECAO', 'NUM_TURNO', 'NUM_VOTAVEL', 'QTDE_VOTOS',\n",
    "       'SECAO_AGREGADA', 'SECAO_AGREGADORA', 'SGL_UF', 'SIGLA_UE', 'SIGLA_UF',\n",
    "       'SITUACAO_LOCAL', 'TIPO_LOCAL', 'index_right', 'precision',\n",
    "    ]\n",
    "    \n",
    "    votes = df.drop(to_drop, axis=1)\n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_centroids_op(df, vote_data, cod_cargo, num_turno, city_maps):\n",
    "    \n",
    "    # Encapsula as funções definidas anteriormente\n",
    "    \n",
    "    votes = filter_incompletes(df)\n",
    "    centroids = make_centroids(votes, city_maps)\n",
    "    centroids = get_centroid_votes(centroids, vote_data, cod_cargo, num_turno, city_maps)\n",
    "    votes = concatenate(votes, centroids)\n",
    "    votes = gpd.GeoDataFrame(votes, geometry=votes.geometry)\n",
    "    votes = clean_cols(votes)\n",
    "    \n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = run_centroids_op(votes, votos_por_secao, \"1\", \"1\", city_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, de posse dos votos, podemos fazer calculos do percentual de votos de cada candidato em cada voronoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_percentage_votes(row):\n",
    "    \n",
    "    # Calcula o percentual dos votos de cada candidato\n",
    "        \n",
    "    all_nums = [col for col in list(row.keys()) if col.isnumeric()] # Votos válidos\n",
    "    party_nums = [col for col in list(row.keys()) if col.isnumeric() and col not in ['95', '96']] # Brancos e nulos\n",
    "    \n",
    "    total_votes = row[party_nums].sum() # Total de votos válidos\n",
    "    total_votes_with_nulls = row[all_nums].sum() # Total de votos incluindo brancos e nulos\n",
    "    \n",
    "    # Dicionário com valores para serem preenchidos\n",
    "    series = {col + \"_PP\":None for col in all_nums}\n",
    "    \n",
    "    for num in all_nums:\n",
    "        \n",
    "        # Calcula percentual dos votos válidos\n",
    "        if num not in ['95', '96']:\n",
    "            try:\n",
    "                percentage = round(row[num] / total_votes, 3)\n",
    "                series[num + \"_PP\"] = percentage\n",
    "            # Se determinado candidato não tiver voto algum neste local, capturar a exceção e preencher com nan\n",
    "            except ZeroDivisionError as e:\n",
    "                series[num + \"_PP\"] = np.nan\n",
    "        \n",
    "        # Aqui, crio variáveis com o percentual de brancos e nulos\n",
    "        # O else é necessário para fazer o cálculo com uma variável diferente\n",
    "        else:\n",
    "            try:\n",
    "                percentage = round(row[num] / total_votes_with_nulls, 3)\n",
    "                series[num + \"_PP\"] = percentage\n",
    "                \n",
    "            # Do mesmo modo que acima, captura exceção caso não haja brancos ou nulos no local\n",
    "            except ZeroDivisionError as e:\n",
    "                series[num + \"_PP\"] = np.nan\n",
    "\n",
    "    return pd.Series(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "votes = pd.concat([votes, votes.apply(get_percentage_votes, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Desenhar diagrama de Voronoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os locais listados, resta desenhar os diagramas de Voronoi. Vamos usar um método baseado [neste](https://github.com/ipython-books/cookbook-2nd-code/blob/master/chapter14_graphgeo/05_voronoi.ipynb) material. O output da função são diversos arquivos .shp, um para cada cidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def voronoi_finite_polygons_2d(vor, radius=None):\n",
    "    \"\"\"Reconstruct infinite Voronoi regions in a\n",
    "    2D diagram to finite regions.\n",
    "    Source:\n",
    "    https://stackoverflow.com/a/20678647/1595060\n",
    "    \"\"\"\n",
    "    if vor.points.shape[1] != 2:\n",
    "        raise ValueError(\"Requires 2D input\")\n",
    "    new_regions = []\n",
    "    new_vertices = vor.vertices.tolist()\n",
    "    center = vor.points.mean(axis=0)\n",
    "    if radius is None:\n",
    "        radius = vor.points.ptp().max()\n",
    "    # Construct a map containing all ridges for a\n",
    "    # given point\n",
    "    all_ridges = {}\n",
    "    for (p1, p2), (v1, v2) in zip(vor.ridge_points,\n",
    "                                  vor.ridge_vertices):\n",
    "        #print(p1, p2)\n",
    "        all_ridges.setdefault(\n",
    "            p1, []).append((p2, v1, v2))\n",
    "        all_ridges.setdefault(\n",
    "            p2, []).append((p1, v1, v2))\n",
    "    # Reconstruct infinite regions\n",
    "    for p1, region in enumerate(vor.point_region):\n",
    "        vertices = vor.regions[region]\n",
    "        if all(v >= 0 for v in vertices):\n",
    "            # finite region\n",
    "            new_regions.append(vertices)\n",
    "            continue\n",
    "        # reconstruct a non-finite region\n",
    "        ridges = all_ridges[p1]\n",
    "        new_region = [v for v in vertices if v >= 0]\n",
    "        for p2, v1, v2 in ridges:\n",
    "            if v2 < 0:\n",
    "                v1, v2 = v2, v1\n",
    "            if v1 >= 0:\n",
    "                # finite ridge: already in the region\n",
    "                continue\n",
    "            # Compute the missing endpoint of an\n",
    "            # infinite ridge\n",
    "            t = vor.points[p2] - \\\n",
    "                vor.points[p1]  # tangent\n",
    "            t /= np.linalg.norm(t)\n",
    "            n = np.array([-t[1], t[0]])  # normal\n",
    "            midpoint = vor.points[[p1, p2]]. \\\n",
    "                mean(axis=0)\n",
    "            direction = np.sign(\n",
    "                np.dot(midpoint - center, n)) * n\n",
    "            far_point = vor.vertices[v2] + \\\n",
    "                direction * radius\n",
    "            new_region.append(len(new_vertices))\n",
    "            new_vertices.append(far_point.tolist())\n",
    "        # Sort region counterclockwise.\n",
    "        vs = np.asarray([new_vertices[v]\n",
    "                         for v in new_region])\n",
    "        c = vs.mean(axis=0)\n",
    "        angles = np.arctan2(\n",
    "            vs[:, 1] - c[1], vs[:, 0] - c[0])\n",
    "        new_region = np.array(new_region)[\n",
    "            np.argsort(angles)]\n",
    "        new_regions.append(new_region.tolist())\n",
    "    return new_regions, np.asarray(new_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_coords(polygons):\n",
    "    \n",
    "    # Reverte as coordenadas que ficaram invertidas após a execução anterior\n",
    "    \n",
    "    temp = []\n",
    "    for polygon in polygons:\n",
    "        temp_polygon = []\n",
    "        for vertice in polygon:\n",
    "            vertice = [vertice[1], vertice[0]]\n",
    "            temp_polygon.append(vertice)\n",
    "        temp_polygon = np.array(temp_polygon)\n",
    "        temp.append(temp_polygon)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_polygons(df):\n",
    "    '''\n",
    "    Função que leva um geo_df como parâmetro e faz merge em todos os polígonos\n",
    "     que compartilham um mesmo id. Isso é necessário porque, depois de recortar um polígono voronoi\n",
    "     contra um contorno de cidade convexo, às vezes acabamos com dois ou mais polígonos para\n",
    "     um único local.\n",
    "    \n",
    "     INPUT: geo_df após calcular a interseção\n",
    "     SAÍDA: geo_df com polígonos mesclados\n",
    "    '''\n",
    "    # Para cada item...\n",
    "    for index, row in df.iterrows():\n",
    "        # Seleciona todos os polígonos com o mesmo id\n",
    "        temp = df[df.id_unico == row.id_unico]\n",
    "        # Se houver mais de um polígono com o mesmo id\n",
    "        if temp.shape[0] > 1:\n",
    "            # Junte todos em apenas um\n",
    "            new_polygon = temp.unary_union\n",
    "            # E subistitua no df original\n",
    "            df.at[index, 'geometry'] = new_polygon\n",
    "    \n",
    "    # Depois, derrube as duplicatas desnecessárias\n",
    "    new_df = df.drop_duplicates(subset='id_unico')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_voronoi_maps(cities, locais, city_maps, exceptions):\n",
    "    '''\n",
    "    INPUT:\n",
    "    cities = array com códigos IBGE\n",
    "    locais = um geo_df com os pontos de cada local de votação\n",
    "    city_maps = geo_df com limites de cidades do IBGE\n",
    "    exeptions = lista vazia que vai ser populada com possíveis erros\n",
    "    '''\n",
    "    count = 0\n",
    "    for index, city in enumerate(tqdm(cities)):\n",
    "        #print('Making city', city, '-', len(cities) - index, 'cities to go')\n",
    "        try:\n",
    "            # Pega o mapa a partir dos arquivos do IBGE\n",
    "            outline = city_maps[city_maps.CD_GEOCMU==city].reset_index()\n",
    "\n",
    "            # Seleciona e salva apenas os pontos correspondentes \n",
    "            points = locais[locais.GEOCOD_IBGE==city].reset_index()\n",
    "            #print(\"Points selected and saved\")\n",
    "\n",
    "            # Se não houver ao menos três pontos, é impossível computar um voronoi.\n",
    "            # Nesse caso, em vez de um voronoi, salvamos o próprio outline da cidade\n",
    "            if points.shape[0] < 4 and points.shape[0] != 1:\n",
    "                print(\"Isso não devia acontecer. Caos!\")\n",
    "                print(points.shape[0], 'pontos detectados')\n",
    "                print(index, city)\n",
    "                return\n",
    "\n",
    "            if points.shape[0] == 1:\n",
    "                count += 1\n",
    "                polygons_df = points.copy()\n",
    "                polygons_df.geometry = outline.geometry\n",
    "                polygons_df.to_file('../data/geo/voronois-merge/' + city + '.shp')\n",
    "\n",
    "            else:\n",
    "                # Salva os pontos encontrados\n",
    "                points.to_file('../data/geo/pontos-cidade/' + city + '.shp')\n",
    "\n",
    "                # Isola as latitudes e longitudes em arrays específicos\n",
    "                lat = points.lat\n",
    "                lon = points.lon\n",
    "\n",
    "                # Usa o pacote voronoi do scipy.spatial\n",
    "                vor = spatial.Voronoi(np.c_[lat, lon])\n",
    "                # Calcula limites finitos para os voronois retornados pelo scipy\n",
    "                regions, vertices = voronoi_finite_polygons_2d(vor, 10000)\n",
    "                # Cria um array com as coordenadas\n",
    "                polygons = [vertices[region] for region in regions]\n",
    "                # Reverte para o formato lon/lat\n",
    "                #print(\"Reversing coordinates\")\n",
    "                polygons = reverse_coords(polygons)\n",
    "                # Transforma o np.array em um polígono do shapely\n",
    "                polygons = [Polygon(polygon) for polygon in polygons]\n",
    "\n",
    "                # Cria um df com o polígono de cada pontos\n",
    "                polygons_df = points.copy()\n",
    "                polygons_df.geometry = polygons\n",
    "\n",
    "                # Salva uma versão pré-clipar\n",
    "                polygons_df.crs = {'init': 'EPSG:4326'}\n",
    "                polygons_df.to_file('../data/geo/voronois-pre-clip/' + city + '.shp')\n",
    "\n",
    "                # Corta os polígonos de voronoi no outline da cidade\n",
    "                # O overlay padrão do geopandas não parece funcionar, gerando alguns bugs inesperados com float precision.\n",
    "                # Vamos ter que fazer dentro de um loop usando a função base do shapely, que opera em apenas um polígono de cada vez.\n",
    "                new_geoseries = []\n",
    "                poly_list = polygons_df.geometry.tolist()\n",
    "                outline_geom = outline.loc[0, 'geometry'].buffer(0) # O buffer(0) corrige self-intersections em quatro municípios\n",
    "                for index, poly in enumerate(poly_list):\n",
    "                    if not (poly.is_valid and outline_geom.is_valid):\n",
    "                        print(\"Problem with geometry validity.\")\n",
    "                        print(\"poly.is_valid:\", poly.is_valid, \"outline_geom.is_valid:\", outline_geom.is_valid)\n",
    "                    intersection = poly.intersection(outline_geom)\n",
    "                    new_geoseries.append(intersection)\n",
    "\n",
    "                # Salva uma versão clipada\n",
    "                polygons_df.geometry = new_geoseries\n",
    "                polygons_df.crs = {'init': 'EPSG:4326'}\n",
    "                polygons_df.to_file('../data/geo/voronois-clipados/' + city + '.shp')\n",
    "\n",
    "\n",
    "                # Após fazer a clipagem, criam-se polígonos isolados, que pertenciam a um voronoi mas acabaram cortados pelo relevo da cidade.\n",
    "                # Eles mantém os campos id_unico e lat_lon_str do voronoi, porém. Podemos reagrupá-los a partir daí.\n",
    "                polygons_df = merge_polygons(polygons_df)\n",
    "                polygons_df.crs = {'init': 'EPSG:4326'}\n",
    "                polygons_df.to_file('../data/geo/voronois-merge/' + city + '.shp')\n",
    "                          \n",
    "        # Captura exceção, printa no console e salva o código da cidade em lista\n",
    "        except Exception as e:\n",
    "            exc_type, exc_obj, tb = sys.exc_info()\n",
    "            f = tb.tb_frame\n",
    "            lineno = tb.tb_lineno\n",
    "            filename = f.f_code.co_filename\n",
    "            linecache.checkcache(filename)\n",
    "            line = linecache.getline(filename, lineno, f.f_globals)\n",
    "            exceptions.append(city)\n",
    "            print(\"Error\", e, \"on city code\", city)\n",
    "            print('EXCEPTION IN', lineno, line)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Roda a função com todos os códigos de cidade.\n",
    "# Note que as duplicatas não são derrubadas in place porque elas são necessárias para reunir votos e voronois posteriormente.\n",
    "cities = votes.GEOCOD_IBGE.unique()\n",
    "exceptions = []\n",
    "make_voronoi_maps( cities, votes, city_maps, exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os arquivos já salvos, precisamos ler e concatenar todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/geo/voronois-merge/*.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voronois = pd.concat([gpd.read_file(file) for file in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, criamos um dicionário de dados com os campos do shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = {\n",
    " 'GEOCOD_IBG': 'COD_IBGE',\n",
    " 'lat_lon_st': 'geom_str',\n",
    " 'local_unic': 'loc_unico',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voronois = voronois.rename(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ligar limites dos setores censitários aos dados\n",
    "Agora vamos ligar os polígonos dos setores censitários aos seus respectivos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, precisamos ler as shapefiles do Censo 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dirs(pattern):\n",
    "    '''\n",
    "    Essa função usa glob e um loop para criar uma lista\n",
    "    dos arquivos que precisamos. Eles estão salvos em uma\n",
    "    estrutura de árvore no diretório, logo um glob simples\n",
    "    não bastaria. De posse da lista, podemos ler os arquivos\n",
    "    todos ao mesmo tempo, simplesmente usando a função built-in\n",
    "    pd.concat([gpd.read(file) for file in files])\n",
    "    '''\n",
    "    directories = glob.glob(pattern)\n",
    "    files = []\n",
    "    for directory in directories:\n",
    "        file = glob.glob(directory + '/*.shp')\n",
    "        files.append(file[0])\n",
    "        \n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feito isso, precisamos colocar os dados do censo no geodataframe – por enquanto, os shapefiles dos setores têm apenas as inforamções geográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os arquivos de dados do censo são dividios em vários formatos, cada um com estatísticas específicas. Vamos ler e padronizar estes grupos de arquivos dentro de um loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Basico_UF:** informações geográficas e quantidade de residentes em domicílios particulares permanentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Domicilio01_UF:** Informações sobre o responsável do domicílio. Vamos usar para extrair dados de mulheres chefes de família."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Domicilio02_UF:** Informações sobre o status do domicílio: é próprio, alugado, cedido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Pessoa01_UF:** Informações sobre a alfabetização dos habitantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Pessoa03_UF:** Informações sobre a raça dos habitantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Pessoa13_UF:** Informações sobre a idade dos habitantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(pattern, correspondence, df_list):\n",
    "    '''\n",
    "    Essa função abre cada um dos arquivos da lista retornada por get_dirs(pattern)\n",
    "    e trata elas de acordo com os parâmetros patterns e correspondence, que serão definidos\n",
    "    antes da chamada da função. Eles devem contar o dicionário de dados do Censo 2010, já\n",
    "    que o cabeçalho dos arquivos contém apenas valores numéricos.\n",
    "    '''\n",
    "    files = glob.glob(pattern)\n",
    "    dtype = {\n",
    "            'Cod_setor':'str',                       \n",
    "            'Cod_Grandes Regiões':'str',                            \n",
    "            'Cod_UF':'str',                                        \n",
    "            'Cod_meso':'str',                                       \n",
    "            'Cod_micro':'str',                                      \n",
    "            'Cod_RM':'str',                                         \n",
    "            'Cod_municipio':'str',                                  \n",
    "            'Cod_distrito':'str',                                   \n",
    "            'Cod_subdistrito':'str',                                \n",
    "            'Cod_bairro':'str',                                     \n",
    "            'Situacao_setor':'str',\n",
    "            'Tipo_setor':'str',                                  \n",
    "            }\n",
    "    fields = list(dtype.keys())\n",
    "    fields.extend(list(correspondence.keys()))\n",
    "    df = pd.concat([pd.read_excel(file, dtype=dtype, usecols_excel=fields) for file in files])\n",
    "    df = df.rename(columns=correspondence)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os arquivos do censo tem campos diferentes e precisam passar por um merge/join para conversar um com os outros. Vamos fazer isso usando a função `reduce`. Uma vez que os dados estejam unidos, é preciso fazer alguns filtros para evitar campos pesados e desnecessários. A função `merge_censo(df_list)` faz tudo isso de uma vez só."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_censo(df_list):\n",
    "    dados_censo = reduce(lambda  left,right: pd.merge(left,right,on='Cod_setor', how='inner', suffixes=('','__y')), df_list)\n",
    "    \n",
    "    # Tira colunas duplicadas pelo merge\n",
    "    to_keep = [column for column in dados_censo.columns if '__y' not in column]\n",
    "    dados_censo = dados_censo[to_keep]\n",
    "    \n",
    "    # Usa um regex com lookahead negativo para manter apenas as colunas que não começam com V\\d{3}\n",
    "    # dados_censo = dados_censo.filter(regex='^(?!V\\d{3}).+', axis=1)\n",
    "    \n",
    "    return dados_censo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há um problema que precisa ser contornado: para determinadas variáveis, em certos setores censitários, o IBGE decidiu anonimizar os dados: em vez de números, os campo têm 'X'. São setores muito pequenos e revelar os dados poderia expor populações sensíveis. Vamos tratá-los como **ZERO**. Como havia um 'X' nesses campos, o Pandas pensa que está lidando com strings em todas as colunas. Não é o caso. Precisamos converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_anom_data(df, correspondences):\n",
    "    # Remove X e substitui por zero\n",
    "    dados_censo = df.replace('X', 0)\n",
    "    \n",
    "    # Pega os campos que deveriam ser numéricos e transforma numa lista de headers.\n",
    "    num_fields = []\n",
    "    for item in correspondences:\n",
    "        num_fields.extend(item.values())\n",
    "        \n",
    "    # Trasnforma estes campos em dtypes numéricos\n",
    "    dados_censo[num_fields] = dados_censo[num_fields].apply(pd.to_numeric, axis=0)\n",
    "    \n",
    "    return dados_censo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função abaixo encapsula todo esse processo e salva o output para um arquivo `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     38,
     48,
     62,
     131,
     140,
     244,
     248
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_censo_concat():\n",
    "    \n",
    "    # 1. Lê arquivos dos setores censitários\n",
    "    files = get_dirs(\"../data/geo/setores-censitarios-ibge/*\")\n",
    "\n",
    "    # 2. Concatena todos os arquivos de setores em apenas um df\n",
    "    setores = pd.concat([gpd.read_file(file) for file in files])\n",
    "  \n",
    "    # 3. Define parâmetros estáticos para as chamadas de função posteriores\n",
    "    patterns = [\n",
    "            \"../data/censo/files/Basico_*\", \n",
    "           '../data/censo/files/Domicilio01_*',\n",
    "           '../data/censo/files/Domicilio02_*',\n",
    "           '../data/censo/files/Pessoa01_*',\n",
    "           '../data/censo/files/Pessoa03_*',\n",
    "           '../data/censo/files/Pessoa13_*',\n",
    "           '../data/censo/files/Pessoa11_*',\n",
    "           '../data/censo/files/Pessoa12_*',\n",
    "            ]\n",
    "   \n",
    "    correspondences = [\n",
    "    \n",
    "        # ARQUIVO_BÁSICO_UF.XLS\n",
    "        {\n",
    "            'V001':'dom_part_permanentes',\n",
    "            'V002':'moradores_dom_part_permanentes',\n",
    "            'V003':'media_num_moradores_dom_part_permanentes',\n",
    "            'V004':'Var_num_moradores_dom_part_permanentes',\n",
    "            'V005':'media_renda_nom_mensal_responsaVeis_dom_part_permanentes',\n",
    "            'V006':'Var_renda_nom_mensal_responsaVeis_dom_part_permanentes',\n",
    "            'V007':'media_renda_nom_mensal_responsaVeis_dom_part_permanentes_com_rendimento',\n",
    "            'V008':'Var_media_renda_nom_mensal_responsaVeis_dom_part_permanentes_com_rendimento',\n",
    "            'V009':'media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento',\n",
    "            'V010':'Var_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento',\n",
    "            'V011':'media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_rendimento',\n",
    "            'V012':'Var_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_rendimento',\n",
    "        },\n",
    "        # ARQUIVO_DOMICÍLIO01_UF.XLS\n",
    "        {\n",
    "            'V081':'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_1_morador',\n",
    "            'V082':'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_2_moradores',\n",
    "            'V083':'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_3_moradores',\n",
    "            'V084':'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_4_moradores',\n",
    "            'V085':'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_5_moradores',\n",
    "            'V086':'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_6_ou_mais_moradores',\n",
    "            'V087':'domicílios_particulares_permanentes_com_mulher_responsáVel_e_sem_outro_morador',\n",
    "        },\n",
    "        # ARQUIVO_DOMICÍLIO02_UF.XLS\n",
    "        {\n",
    "            'V001':'moradores_em_domicílios_particulares_e_domicílios_coletivos',\n",
    "            'V002':'moradores_em_domicílios_particulares_permanentes',\n",
    "            'V003':'moradores_em_domicílios_particulares_permanentes_do_tipo_casa',\n",
    "            'V004':'moradores_em_domicílios_particulares_permanentes_do_tipo_casa_de_vila_ou_em_condomínio',\n",
    "            'V005':'moradores_em_domicílios_particulares_permanentes_do_tipo_apartamento',\n",
    "            'V006':'moradores_em_domicílios_particulares_permanentes_próprios_e_quitados',\n",
    "            'V007':'moradores_em_domicílios_particulares_permanentes_próprios_e_em_aquisição',\n",
    "            'V008':'moradores_em_domicílios_particulares_permanentes_alugados',\n",
    "            'V009':'moradores_em_domicílios_particulares_permanentes_cedidos_por_empregador',\n",
    "            'V010':'moradores_em_domicílios_particulares_permanentes_cedidos_de_outra_forma',\n",
    "            'V011':'moradores_em_domicílios_particulares_permanentes_com_outra_condição_de_ocupação',\n",
    "        },\n",
    "        # ARQUIVO_PESSOA01_UF\n",
    "        {\n",
    "            'V001':'pessoas_alfabetizadas_com_5_ou_mais_anos_de_idade',\n",
    "            'V013':'pessoas_alfabetizadas_com_16_anos_de_idade',\n",
    "            'V014':'pessoas_alfabetizadas_com_17_anos_de_idade',\n",
    "            'V015':'pessoas_alfabetizadas_com_18_anos_de_idade',\n",
    "            'V016':'pessoas_alfabetizadas_com_19_anos_de_idade',\n",
    "            'V017':'pessoas_alfabetizadas_com_20_anos_de_idade',\n",
    "            'V018':'pessoas_alfabetizadas_com_21_anos_de_idade',\n",
    "            'V019':'pessoas_alfabetizadas_com_22_anos_de_idade',\n",
    "            'V020':'pessoas_alfabetizadas_com_23_anos_de_idade',\n",
    "            'V021':'pessoas_alfabetizadas_com_24_anos_de_idade',\n",
    "            'V022':'pessoas_alfabetizadas_com_25_anos_de_idade',\n",
    "            'V023':'pessoas_alfabetizadas_com_26_anos_de_idade',\n",
    "            'V024':'pessoas_alfabetizadas_com_27_anos_de_idade',\n",
    "            'V025':'pessoas_alfabetizadas_com_28_anos_de_idade',\n",
    "            'V026':'pessoas_alfabetizadas_com_29_anos_de_idade',\n",
    "            'V027':'pessoas_alfabetizadas_com_30_anos_de_idade',\n",
    "            'V028':'pessoas_alfabetizadas_com_31_anos_de_idade',\n",
    "            'V029':'pessoas_alfabetizadas_com_32_anos_de_idade',\n",
    "            'V030':'pessoas_alfabetizadas_com_33_anos_de_idade',\n",
    "            'V031':'pessoas_alfabetizadas_com_34_anos_de_idade',\n",
    "            'V032':'pessoas_alfabetizadas_com_35_anos_de_idade',\n",
    "            'V033':'pessoas_alfabetizadas_com_36_anos_de_idade',\n",
    "            'V034':'pessoas_alfabetizadas_com_37_anos_de_idade',\n",
    "            'V035':'pessoas_alfabetizadas_com_38_anos_de_idade',\n",
    "            'V036':'pessoas_alfabetizadas_com_39_anos_de_idade',\n",
    "            'V037':'pessoas_alfabetizadas_com_40_anos_de_idade',\n",
    "            'V038':'pessoas_alfabetizadas_com_41_anos_de_idade',\n",
    "            'V039':'pessoas_alfabetizadas_com_42_anos_de_idade',\n",
    "            'V040':'pessoas_alfabetizadas_com_43_anos_de_idade',\n",
    "            'V041':'pessoas_alfabetizadas_com_44_anos_de_idade',\n",
    "            'V042':'pessoas_alfabetizadas_com_45_anos_de_idade',\n",
    "            'V043':'pessoas_alfabetizadas_com_46_anos_de_idade',\n",
    "            'V044':'pessoas_alfabetizadas_com_47_anos_de_idade',\n",
    "            'V045':'pessoas_alfabetizadas_com_48_anos_de_idade',\n",
    "            'V046':'pessoas_alfabetizadas_com_49_anos_de_idade',\n",
    "            'V047':'pessoas_alfabetizadas_com_50_anos_de_idade',\n",
    "            'V048':'pessoas_alfabetizadas_com_51_anos_de_idade',\n",
    "            'V049':'pessoas_alfabetizadas_com_52_anos_de_idade',\n",
    "            'V050':'pessoas_alfabetizadas_com_53_anos_de_idade',\n",
    "            'V051':'pessoas_alfabetizadas_com_54_anos_de_idade',\n",
    "            'V052':'pessoas_alfabetizadas_com_55_anos_de_idade',\n",
    "            'V053':'pessoas_alfabetizadas_com_56_anos_de_idade',\n",
    "            'V054':'pessoas_alfabetizadas_com_57_anos_de_idade',\n",
    "            'V055':'pessoas_alfabetizadas_com_58_anos_de_idade',\n",
    "            'V056':'pessoas_alfabetizadas_com_59_anos_de_idade',\n",
    "            'V057':'pessoas_alfabetizadas_com_60_anos_de_idade',\n",
    "            'V058':'pessoas_alfabetizadas_com_61_anos_de_idade',\n",
    "            'V059':'pessoas_alfabetizadas_com_62_anos_de_idade',\n",
    "            'V060':'pessoas_alfabetizadas_com_63_anos_de_idade',\n",
    "            'V061':'pessoas_alfabetizadas_com_64_anos_de_idade',\n",
    "            'V062':'pessoas_alfabetizadas_com_65_anos_de_idade',\n",
    "            'V063':'pessoas_alfabetizadas_com_66_anos_de_idade',\n",
    "            'V064':'pessoas_alfabetizadas_com_67_anos_de_idade',\n",
    "            'V065':'pessoas_alfabetizadas_com_68_anos_de_idade',\n",
    "            'V066':'pessoas_alfabetizadas_com_69_anos_de_idade',\n",
    "            'V067':'pessoas_alfabetizadas_com_70_anos_de_idade',\n",
    "            'V068':'pessoas_alfabetizadas_com_71_anos_de_idade',\n",
    "            'V069':'pessoas_alfabetizadas_com_72_anos_de_idade',\n",
    "            'V070':'pessoas_alfabetizadas_com_73_anos_de_idade',\n",
    "            'V071':'pessoas_alfabetizadas_com_74_anos_de_idade',\n",
    "            'V072':'pessoas_alfabetizadas_com_75_anos_de_idade',\n",
    "            'V073':'pessoas_alfabetizadas_com_76_anos_de_idade',\n",
    "            'V074':'pessoas_alfabetizadas_com_77_anos_de_idade',\n",
    "            'V075':'pessoas_alfabetizadas_com_78_anos_de_idade',\n",
    "            'V076':'pessoas_alfabetizadas_com_79_anos_de_idade',\n",
    "            'V077':'pessoas_alfabetizadas_com_80_anos_ou_mais_de_idade',\n",
    "        },\n",
    "        # ARQUIVO_PESSOA03_UF.XLS\n",
    "        {\n",
    "            'V001':'pessoas_residentes',\n",
    "            'V002':'pessoas_residentes_e_cor_ou_raça_branca',\n",
    "            'V003':'pessoas_residentes_e_cor_ou_raça_preta',\n",
    "            'V004':'pessoas_residentes_e_cor_ou_raça_amarela',\n",
    "            'V005':'pessoas_residentes_e_cor_ou_raça_parda',\n",
    "            'V006':'pessoas_residentes_e_cor_ou_raça_indígenas',\n",
    "        },\n",
    "        # ARQUIVO_PESSOA13_UF.XLS\n",
    "        {\n",
    "            'V022':'pessoas_com_menos_de_1_ano_de_idade',\n",
    "            'V035':'pessoas_de_1_ano_de_idade',\n",
    "            'V036':'pessoas_com_2_anos_de_idade',\n",
    "            'V037':'pessoas_com_3_anos_de_idade',\n",
    "            'V038':'pessoas_com_4_anos_de_idade',\n",
    "            'V039':'pessoas_com_5_anos_de_idade',\n",
    "            'V040':'pessoas_com_6_anos_de_idade',\n",
    "            'V041':'pessoas_com_7_anos_de_idade',\n",
    "            'V042':'pessoas_com_8_anos_de_idade',\n",
    "            'V043':'pessoas_com_9_anos_de_idade',\n",
    "            'V044':'pessoas_com_10_anos_de_idade',\n",
    "            'V045':'pessoas_com_11_anos_de_idade',\n",
    "            'V046':'pessoas_com_12_anos_de_idade',\n",
    "            'V047':'pessoas_com_13_anos_de_idade',\n",
    "            'V048':'pessoas_com_14_anos_de_idade',\n",
    "            'V049':'pessoas_com_15_anos_de_idade',\n",
    "            'V050':'pessoas_com_16_anos_de_idade',\n",
    "            'V051':'pessoas_com_17_anos_de_idade',\n",
    "            'V052':'pessoas_com_18_anos_de_idade',\n",
    "            'V053':'pessoas_com_19_anos_de_idade',\n",
    "            'V054':'pessoas_com_20_anos_de_idade',\n",
    "            'V055':'pessoas_com_21_anos_de_idade',\n",
    "            'V056':'pessoas_com_22_anos_de_idade',\n",
    "            'V057':'pessoas_com_23_anos_de_idade',\n",
    "            'V058':'pessoas_com_24_anos_de_idade',\n",
    "            'V059':'pessoas_com_25_anos_de_idade',\n",
    "            'V060':'pessoas_com_26_anos_de_idade',\n",
    "            'V061':'pessoas_com_27_anos_de_idade',\n",
    "            'V062':'pessoas_com_28_anos_de_idade',\n",
    "            'V063':'pessoas_com_29_anos_de_idade',\n",
    "            'V064':'pessoas_com_30_anos_de_idade',\n",
    "            'V065':'pessoas_com_31_anos_de_idade',\n",
    "            'V066':'pessoas_com_32_anos_de_idade',\n",
    "            'V067':'pessoas_com_33_anos_de_idade',\n",
    "            'V068':'pessoas_com_34_anos_de_idade',\n",
    "            'V069':'pessoas_com_35_anos_de_idade',\n",
    "            'V070':'pessoas_com_36_anos_de_idade',\n",
    "            'V071':'pessoas_com_37_anos_de_idade',\n",
    "            'V072':'pessoas_com_38_anos_de_idade',\n",
    "            'V073':'pessoas_com_39_anos_de_idade',\n",
    "            'V074':'pessoas_com_40_anos_de_idade',\n",
    "            'V075':'pessoas_com_41_anos_de_idade',\n",
    "            'V076':'pessoas_com_42_anos_de_idade',\n",
    "            'V077':'pessoas_com_43_anos_de_idade',\n",
    "            'V078':'pessoas_com_44_anos_de_idade',\n",
    "            'V079':'pessoas_com_45_anos_de_idade',\n",
    "            'V080':'pessoas_com_46_anos_de_idade',\n",
    "            'V081':'pessoas_com_47_anos_de_idade',\n",
    "            'V082':'pessoas_com_48_anos_de_idade',\n",
    "            'V083':'pessoas_com_49_anos_de_idade',\n",
    "            'V084':'pessoas_com_50_anos_de_idade',\n",
    "            'V085':'pessoas_com_51_anos_de_idade',\n",
    "            'V086':'pessoas_com_52_anos_de_idade',\n",
    "            'V087':'pessoas_com_53_anos_de_idade',\n",
    "            'V088':'pessoas_com_54_anos_de_idade',\n",
    "            'V089':'pessoas_com_55_anos_de_idade',\n",
    "            'V090':'pessoas_com_56_anos_de_idade',\n",
    "            'V091':'pessoas_com_57_anos_de_idade',\n",
    "            'V092':'pessoas_com_58_anos_de_idade',\n",
    "            'V093':'pessoas_com_59_anos_de_idade',\n",
    "            'V094':'pessoas_com_60_anos_de_idade',\n",
    "            'V095':'pessoas_com_61_anos_de_idade',\n",
    "            'V096':'pessoas_com_62_anos_de_idade',\n",
    "            'V097':'pessoas_com_63_anos_de_idade',\n",
    "            'V098':'pessoas_com_64_anos_de_idade',\n",
    "            'V099':'pessoas_com_65_anos_de_idade',\n",
    "            'V100':'pessoas_com_66_anos_de_idade',\n",
    "            'V101':'pessoas_com_67_anos_de_idade',\n",
    "            'V102':'pessoas_com_68_anos_de_idade',\n",
    "            'V103':'pessoas_com_69_anos_de_idade',\n",
    "            'V104':'pessoas_com_70_anos_de_idade',\n",
    "            'V105':'pessoas_com_71_anos_de_idade',\n",
    "            'V106':'pessoas_com_72_anos_de_idade',\n",
    "            'V107':'pessoas_com_73_anos_de_idade',\n",
    "            'V108':'pessoas_com_74_anos_de_idade',\n",
    "            'V109':'pessoas_com_75_anos_de_idade',\n",
    "            'V110':'pessoas_com_76_anos_de_idade',\n",
    "            'V111':'pessoas_com_77_anos_de_idade',\n",
    "            'V112':'pessoas_com_78_anos_de_idade',\n",
    "            'V113':'pessoas_com_79_anos_de_idade',\n",
    "            'V114':'pessoas_com_80_anos_de_idade',\n",
    "            'V115':'pessoas_com_81_anos_de_idade',\n",
    "            'V116':'pessoas_com_82_anos_de_idade',\n",
    "            'V117':'pessoas_com_83_anos_de_idade',\n",
    "            'V118':'pessoas_com_84_anos_de_idade',\n",
    "            'V119':'pessoas_com_85_anos_de_idade',\n",
    "            'V120':'pessoas_com_86_anos_de_idade',\n",
    "            'V121':'pessoas_com_87_anos_de_idade',\n",
    "            'V122':'pessoas_com_88_anos_de_idade',\n",
    "            'V123':'pessoas_com_89_anos_de_idade',\n",
    "            'V124':'pessoas_com_90_anos_de_idade',\n",
    "            'V125':'pessoas_com_91_anos_de_idade',\n",
    "            'V126':'pessoas_com_92_anos_de_idade',\n",
    "            'V127':'pessoas_com_93_anos_de_idade',\n",
    "            'V128':'pessoas_com_94_anos_de_idade',\n",
    "            'V129':'pessoas_com_95_anos_de_idade',\n",
    "            'V130':'pessoas_com_96_anos_de_idade',\n",
    "            'V131':'pessoas_com_97_anos_de_idade',\n",
    "            'V132':'pessoas_com_98_anos_de_idade',\n",
    "            'V133':'pessoas_com_99_anos_de_idade',\n",
    "            'V134':'pessoas_com_100_anos_ou_mais_de_idade',\n",
    "        },\n",
    "        # ARQUIVO_PESSOA11_UF.XLS    \n",
    "        {\n",
    "            'V001':'homens_residentes_em_domicilios_particulares_e_domicilios_coletivos'\n",
    "        },\n",
    "        # ARQUIVO_PESSOA12_UF.XLS\n",
    "        {\n",
    "            'V001':'mulheres_residentes_em_domicilios_particulares_e_domicilios_coletivos'\n",
    "        },    \n",
    "    ]\n",
    "    \n",
    "    # 4. Lê cada um dos arquivos do censo de acordo com os parâmetros previamente definidos\n",
    "    dados_censo = []\n",
    "    for pattern, correspondence in zip(patterns, correspondences):\n",
    "        read_data(pattern, correspondence, dados_censo)\n",
    "        \n",
    "    # 5. Reduce itera par a par nos itens da lista que foi preenchida por read_data. Depois, o merge é feito com lambda\n",
    "    dados_censo = merge_censo(dados_censo)\n",
    "    \n",
    "    # 6. Resolve caso dos campos com 'X' para setores anonimizados\n",
    "    dados_censo = handle_anom_data(dados_censo, correspondences)\n",
    "    \n",
    "    # 7. Salva para reiniciar sem esperar ler e dar merge em todos os arquivos, quando necessário\n",
    "    dados_censo.to_csv(\"../data/dados-censo-concatenados-temp.csv\", index=False)\n",
    "    setores.to_file(\"../data/geo/setores-concatenados-temp.shp\")\n",
    "    \n",
    "    return dados_censo, setores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos rodá-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dados_censo, setores = run_censo_concat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função anterior salvou um arquivo `.csv` e um `.shp`. Vamos abrí-los. Assim, evitamos o tempo de carregamento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Lê novamente\n",
    "dtype = {\n",
    "            'Cod_setor':'str',                       \n",
    "            'Cod_Grandes Regiões':'str',                            \n",
    "            'Cod_UF':'str',                                        \n",
    "            'Cod_meso':'str',                                       \n",
    "            'Cod_micro':'str',                                      \n",
    "            'Cod_RM':'str',                                         \n",
    "            'Cod_municipio':'str',                                  \n",
    "            'Cod_distrito':'str',                                   \n",
    "            'Cod_subdistrito':'str',                                \n",
    "            'Cod_bairro':'str',                                     \n",
    "            'Situacao_setor':'str',\n",
    "            'Tipo_setor':'str',                                  \n",
    "        }\n",
    "dados_censo = pd.read_csv(\"../data/dados-censo-concatenados-temp.csv\", dtype=dtype)\n",
    "setores = gpd.read_file(\"../data/geo/setores-concatenados-temp.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Criar campos percentuais para os valores de cada setor censitário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável `dados_censo` é um grande dataframe com informações úteis. Entretanto, elas estão mais granulares do que gostaríamos. Vamos definir funções para extrair dados mais consolidados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos:\n",
    "- Total de eleitores jovens (16 - 24 anos)\n",
    "- Total de pessoas alfabetizadas\n",
    "- Total de eleitores alfabetizados\n",
    "- Total de eleitores mulheres\n",
    "- Total de eleitores homens\n",
    "- Total de mulheres responsáveis por domicílio\n",
    "- Total de negros, brancos, pardos, amarelos e indígenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função abaixo calcula esses percentuais. Ela recebe uma lista de campos para somar e o nome do campo que será retornado. Vamos usar ela em um loop para salvar tudo que precisamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_aggregates(fields_to_sum, agg_field_name, df):\n",
    "        total_of_group = df[fields_to_sum].sum(axis=1)\n",
    "        df[agg_field_name] = total_of_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     94,
     106,
     110,
     182,
     212,
     232
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_aggregation(df):\n",
    "    \n",
    "    # Cópia para não alterar localmente\n",
    "    temp_df = df.copy()\n",
    "\n",
    "    # Define as variáveis necessárias para agregar\n",
    "    pessoas_em_idade_eleitoral = [ \n",
    "            'pessoas_com_16_anos_de_idade',\n",
    "            'pessoas_com_17_anos_de_idade',\n",
    "            'pessoas_com_18_anos_de_idade',\n",
    "            'pessoas_com_19_anos_de_idade',\n",
    "            'pessoas_com_20_anos_de_idade',\n",
    "            'pessoas_com_21_anos_de_idade',\n",
    "            'pessoas_com_22_anos_de_idade',\n",
    "            'pessoas_com_23_anos_de_idade',\n",
    "            'pessoas_com_24_anos_de_idade',\n",
    "            'pessoas_com_25_anos_de_idade',\n",
    "            'pessoas_com_26_anos_de_idade',\n",
    "            'pessoas_com_27_anos_de_idade',\n",
    "            'pessoas_com_28_anos_de_idade',\n",
    "            'pessoas_com_29_anos_de_idade',\n",
    "            'pessoas_com_30_anos_de_idade',\n",
    "            'pessoas_com_31_anos_de_idade',\n",
    "            'pessoas_com_32_anos_de_idade',\n",
    "            'pessoas_com_33_anos_de_idade',\n",
    "            'pessoas_com_34_anos_de_idade',\n",
    "            'pessoas_com_35_anos_de_idade',\n",
    "            'pessoas_com_36_anos_de_idade',\n",
    "            'pessoas_com_37_anos_de_idade',\n",
    "            'pessoas_com_38_anos_de_idade',\n",
    "            'pessoas_com_39_anos_de_idade',\n",
    "            'pessoas_com_40_anos_de_idade',\n",
    "            'pessoas_com_41_anos_de_idade',\n",
    "            'pessoas_com_42_anos_de_idade',\n",
    "            'pessoas_com_43_anos_de_idade',\n",
    "            'pessoas_com_44_anos_de_idade',\n",
    "            'pessoas_com_45_anos_de_idade',\n",
    "            'pessoas_com_46_anos_de_idade',\n",
    "            'pessoas_com_47_anos_de_idade',\n",
    "            'pessoas_com_48_anos_de_idade',\n",
    "            'pessoas_com_49_anos_de_idade',\n",
    "            'pessoas_com_50_anos_de_idade',\n",
    "            'pessoas_com_51_anos_de_idade',\n",
    "            'pessoas_com_52_anos_de_idade',\n",
    "            'pessoas_com_53_anos_de_idade',\n",
    "            'pessoas_com_54_anos_de_idade',\n",
    "            'pessoas_com_55_anos_de_idade',\n",
    "            'pessoas_com_56_anos_de_idade',\n",
    "            'pessoas_com_57_anos_de_idade',\n",
    "            'pessoas_com_58_anos_de_idade',\n",
    "            'pessoas_com_59_anos_de_idade',\n",
    "            'pessoas_com_60_anos_de_idade',\n",
    "            'pessoas_com_61_anos_de_idade',\n",
    "            'pessoas_com_62_anos_de_idade',\n",
    "            'pessoas_com_63_anos_de_idade',\n",
    "            'pessoas_com_64_anos_de_idade',\n",
    "            'pessoas_com_65_anos_de_idade',\n",
    "            'pessoas_com_66_anos_de_idade',\n",
    "            'pessoas_com_67_anos_de_idade',\n",
    "            'pessoas_com_68_anos_de_idade',\n",
    "            'pessoas_com_69_anos_de_idade',\n",
    "            'pessoas_com_70_anos_de_idade',\n",
    "            'pessoas_com_71_anos_de_idade',\n",
    "            'pessoas_com_72_anos_de_idade',\n",
    "            'pessoas_com_73_anos_de_idade',\n",
    "            'pessoas_com_74_anos_de_idade',\n",
    "            'pessoas_com_75_anos_de_idade',\n",
    "            'pessoas_com_76_anos_de_idade',\n",
    "            'pessoas_com_77_anos_de_idade',\n",
    "            'pessoas_com_78_anos_de_idade',\n",
    "            'pessoas_com_79_anos_de_idade',\n",
    "            'pessoas_com_80_anos_de_idade',\n",
    "            'pessoas_com_81_anos_de_idade',\n",
    "            'pessoas_com_82_anos_de_idade',\n",
    "            'pessoas_com_83_anos_de_idade',\n",
    "            'pessoas_com_84_anos_de_idade',\n",
    "            'pessoas_com_85_anos_de_idade',\n",
    "            'pessoas_com_86_anos_de_idade',\n",
    "            'pessoas_com_87_anos_de_idade',\n",
    "            'pessoas_com_88_anos_de_idade',\n",
    "            'pessoas_com_89_anos_de_idade',\n",
    "            'pessoas_com_90_anos_de_idade',\n",
    "            'pessoas_com_91_anos_de_idade',\n",
    "            'pessoas_com_92_anos_de_idade',\n",
    "            'pessoas_com_93_anos_de_idade',\n",
    "            'pessoas_com_94_anos_de_idade',\n",
    "            'pessoas_com_95_anos_de_idade',\n",
    "            'pessoas_com_96_anos_de_idade',\n",
    "            'pessoas_com_97_anos_de_idade',\n",
    "            'pessoas_com_98_anos_de_idade',\n",
    "            'pessoas_com_99_anos_de_idade',\n",
    "            'pessoas_com_100_anos_ou_mais_de_idade' \n",
    "        ]\n",
    "\n",
    "    eleitores_de_16_a_24_anos = [ \n",
    "                'pessoas_com_16_anos_de_idade',\n",
    "                'pessoas_com_17_anos_de_idade',\n",
    "                'pessoas_com_18_anos_de_idade',\n",
    "                'pessoas_com_19_anos_de_idade',\n",
    "                'pessoas_com_20_anos_de_idade',\n",
    "                'pessoas_com_21_anos_de_idade',\n",
    "                'pessoas_com_22_anos_de_idade',\n",
    "                'pessoas_com_23_anos_de_idade',\n",
    "                'pessoas_com_24_anos_de_idade',\n",
    "        ]\n",
    "\n",
    "    total_alfabetizados = [\n",
    "                            'pessoas_alfabetizadas_com_5_ou_mais_anos_de_idade'\n",
    "                          ]\n",
    "\n",
    "    eleitores_alfabetizados = [\n",
    "                                'pessoas_alfabetizadas_com_16_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_17_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_18_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_19_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_20_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_21_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_22_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_23_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_24_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_25_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_26_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_27_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_28_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_29_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_30_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_31_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_32_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_33_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_34_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_35_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_36_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_37_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_38_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_39_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_40_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_41_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_42_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_43_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_44_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_45_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_46_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_47_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_48_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_49_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_50_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_51_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_52_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_53_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_54_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_55_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_56_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_57_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_58_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_59_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_60_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_61_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_62_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_63_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_64_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_65_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_66_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_67_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_68_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_69_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_70_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_71_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_72_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_73_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_74_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_75_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_76_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_77_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_78_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_79_anos_de_idade',\n",
    "                                'pessoas_alfabetizadas_com_80_anos_ou_mais_de_idade',\n",
    "                            ]\n",
    "\n",
    "    total_homens = ['homens_residentes_em_domicilios_particulares_e_domicilios_coletivos']\n",
    "\n",
    "    total_mulheres = ['mulheres_residentes_em_domicilios_particulares_e_domicilios_coletivos']\n",
    "\n",
    "    total_mulheres_responsaveis_por_domicilio = [ \n",
    "                                                 'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_1_morador',\n",
    "                                                 'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_2_moradores',\n",
    "                                                 'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_3_moradores',\n",
    "                                                 'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_4_moradores',\n",
    "                                                 'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_5_moradores',\n",
    "                                                 'domicílios_particulares_permanentes_com_mulher_responsável_e_mais_6_ou_mais_moradores',\n",
    "                                                 'domicílios_particulares_permanentes_com_mulher_responsáVel_e_sem_outro_morador'\n",
    "                                                ]\n",
    "\n",
    "    total_pretos = ['pessoas_residentes_e_cor_ou_raça_preta']\n",
    "\n",
    "    total_pardos = ['pessoas_residentes_e_cor_ou_raça_parda']\n",
    "\n",
    "    total_pretos_e_pardos = ['pessoas_residentes_e_cor_ou_raça_preta', 'pessoas_residentes_e_cor_ou_raça_parda']\n",
    "\n",
    "    total_brancos = ['pessoas_residentes_e_cor_ou_raça_branca']\n",
    "\n",
    "    total_indigenas = ['pessoas_residentes_e_cor_ou_raça_indígenas']\n",
    "\n",
    "    total_amarelos = ['pessoas_residentes_e_cor_ou_raça_amarela']\n",
    "    \n",
    "    # Lista os campos que precisam ser somados - é uma lista das listas criadas acima\n",
    "    fields_to_sum = [ \n",
    "        pessoas_em_idade_eleitoral, eleitores_de_16_a_24_anos, total_alfabetizados, eleitores_alfabetizados, total_homens, total_mulheres, \n",
    "        total_mulheres_responsaveis_por_domicilio, total_pretos, total_pardos, total_pretos_e_pardos,\n",
    "        total_brancos, total_indigenas, total_amarelos, \n",
    "    ]\n",
    "    \n",
    "    # Define as labels dos resultados\n",
    "    result_labels = [ \n",
    "     \"pessoas_em_idade_eleitoral\", \"eleitores_de_16_a_24_anos\", \"total_alfabetizados\", \"eleitores_alfabetizados\", \"total_homens\", \"total_mulheres\", \n",
    "     \"total_mulheres_responsaveis_por_domicilio\", \"total_pretos\", \"total_pardos\", \"total_pretos_e_pardos\",\n",
    "     \"total_brancos\", \"total_indigenas\", \"total_amarelos\", \n",
    "    ]\n",
    "    \n",
    "    # Roda a função, modificando o df inplace\n",
    "    for fields, result_label in zip(fields_to_sum, result_labels):\n",
    "        get_aggregates(fields, result_label, temp_df)\n",
    "        \n",
    "    # Filtra, novamente inplace, para manter apenas as colunas relevantes\n",
    "    to_keep = [\n",
    "        'Cod_setor', 'Cod_Grandes Regiões', 'Nome_Grande_Regiao', 'Cod_UF',\n",
    "        'Nome_da_UF ', 'Cod_meso', 'Nome_da_meso', 'Cod_micro', 'Nome_da_micro',\n",
    "        'Cod_RM', 'Nome_da_RM', 'Cod_municipio', 'Nome_do_municipio',\n",
    "        'Cod_distrito', 'Nome_do_distrito', 'Cod_subdistrito',\n",
    "        'Nome_do_subdistrito', 'Cod_bairro', 'Nome_do_bairro', 'Situacao_setor',\n",
    "        'Tipo_setor', 'pessoas_residentes', \n",
    "        'media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento',\n",
    "    ]\n",
    "    ages = [\n",
    "            'pessoas_com_menos_de_1_ano_de_idade',\n",
    "            'pessoas_de_1_ano_de_idade',\n",
    "            'pessoas_com_2_anos_de_idade',\n",
    "            'pessoas_com_3_anos_de_idade',\n",
    "            'pessoas_com_4_anos_de_idade',\n",
    "            'pessoas_com_5_anos_de_idade',\n",
    "            'pessoas_com_6_anos_de_idade',\n",
    "            'pessoas_com_7_anos_de_idade',\n",
    "            'pessoas_com_8_anos_de_idade',\n",
    "            'pessoas_com_9_anos_de_idade',\n",
    "            'pessoas_com_10_anos_de_idade',\n",
    "            'pessoas_com_11_anos_de_idade',\n",
    "            'pessoas_com_12_anos_de_idade',\n",
    "            'pessoas_com_13_anos_de_idade',\n",
    "            'pessoas_com_14_anos_de_idade',\n",
    "            'pessoas_com_15_anos_de_idade',\n",
    "            'pessoas_com_16_anos_de_idade',\n",
    "            'pessoas_com_17_anos_de_idade',\n",
    "            'pessoas_com_18_anos_de_idade',\n",
    "            'pessoas_com_19_anos_de_idade',\n",
    "            'pessoas_com_20_anos_de_idade',\n",
    "            'pessoas_com_21_anos_de_idade',\n",
    "            'pessoas_com_22_anos_de_idade',\n",
    "            'pessoas_com_23_anos_de_idade',\n",
    "            'pessoas_com_24_anos_de_idade',\n",
    "            'pessoas_com_25_anos_de_idade',\n",
    "            'pessoas_com_26_anos_de_idade',\n",
    "            'pessoas_com_27_anos_de_idade',\n",
    "            'pessoas_com_28_anos_de_idade',\n",
    "            'pessoas_com_29_anos_de_idade',\n",
    "            'pessoas_com_30_anos_de_idade',\n",
    "            'pessoas_com_31_anos_de_idade',\n",
    "            'pessoas_com_32_anos_de_idade',\n",
    "            'pessoas_com_33_anos_de_idade',\n",
    "            'pessoas_com_34_anos_de_idade',\n",
    "            'pessoas_com_35_anos_de_idade',\n",
    "            'pessoas_com_36_anos_de_idade',\n",
    "            'pessoas_com_37_anos_de_idade',\n",
    "            'pessoas_com_38_anos_de_idade',\n",
    "            'pessoas_com_39_anos_de_idade',\n",
    "            'pessoas_com_40_anos_de_idade',\n",
    "            'pessoas_com_41_anos_de_idade',\n",
    "            'pessoas_com_42_anos_de_idade',\n",
    "            'pessoas_com_43_anos_de_idade',\n",
    "            'pessoas_com_44_anos_de_idade',\n",
    "            'pessoas_com_45_anos_de_idade',\n",
    "            'pessoas_com_46_anos_de_idade',\n",
    "            'pessoas_com_47_anos_de_idade',\n",
    "            'pessoas_com_48_anos_de_idade',\n",
    "            'pessoas_com_49_anos_de_idade',\n",
    "            'pessoas_com_50_anos_de_idade',\n",
    "            'pessoas_com_51_anos_de_idade',\n",
    "            'pessoas_com_52_anos_de_idade',\n",
    "            'pessoas_com_53_anos_de_idade',\n",
    "            'pessoas_com_54_anos_de_idade',\n",
    "            'pessoas_com_55_anos_de_idade',\n",
    "            'pessoas_com_56_anos_de_idade',\n",
    "            'pessoas_com_57_anos_de_idade',\n",
    "            'pessoas_com_58_anos_de_idade',\n",
    "            'pessoas_com_59_anos_de_idade',\n",
    "            'pessoas_com_60_anos_de_idade',\n",
    "            'pessoas_com_61_anos_de_idade',\n",
    "            'pessoas_com_62_anos_de_idade',\n",
    "            'pessoas_com_63_anos_de_idade',\n",
    "            'pessoas_com_64_anos_de_idade',\n",
    "            'pessoas_com_65_anos_de_idade',\n",
    "            'pessoas_com_66_anos_de_idade',\n",
    "            'pessoas_com_67_anos_de_idade',\n",
    "            'pessoas_com_68_anos_de_idade',\n",
    "            'pessoas_com_69_anos_de_idade',\n",
    "            'pessoas_com_70_anos_de_idade',\n",
    "            'pessoas_com_71_anos_de_idade',\n",
    "            'pessoas_com_72_anos_de_idade',\n",
    "            'pessoas_com_73_anos_de_idade',\n",
    "            'pessoas_com_74_anos_de_idade',\n",
    "            'pessoas_com_75_anos_de_idade',\n",
    "            'pessoas_com_76_anos_de_idade',\n",
    "            'pessoas_com_77_anos_de_idade',\n",
    "            'pessoas_com_78_anos_de_idade',\n",
    "            'pessoas_com_79_anos_de_idade',\n",
    "            'pessoas_com_80_anos_de_idade',\n",
    "            'pessoas_com_81_anos_de_idade',\n",
    "            'pessoas_com_82_anos_de_idade',\n",
    "            'pessoas_com_83_anos_de_idade',\n",
    "            'pessoas_com_84_anos_de_idade',\n",
    "            'pessoas_com_85_anos_de_idade',\n",
    "            'pessoas_com_86_anos_de_idade',\n",
    "            'pessoas_com_87_anos_de_idade',\n",
    "            'pessoas_com_88_anos_de_idade',\n",
    "            'pessoas_com_89_anos_de_idade',\n",
    "            'pessoas_com_90_anos_de_idade',\n",
    "            'pessoas_com_91_anos_de_idade',\n",
    "            'pessoas_com_92_anos_de_idade',\n",
    "            'pessoas_com_93_anos_de_idade',\n",
    "            'pessoas_com_94_anos_de_idade',\n",
    "            'pessoas_com_95_anos_de_idade',\n",
    "            'pessoas_com_96_anos_de_idade',\n",
    "            'pessoas_com_97_anos_de_idade',\n",
    "            'pessoas_com_98_anos_de_idade',\n",
    "            'pessoas_com_99_anos_de_idade',\n",
    "            'pessoas_com_100_anos_ou_mais_de_idade',\n",
    "        ]\n",
    "    to_keep.extend(result_labels)\n",
    "    to_keep.extend(ages)    \n",
    "    temp_df = temp_df[to_keep]\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dados_censo = run_aggregation(dados_censo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos unir esses dados aos shapefiles com base nos campos `CD_GEOCODI` e `Cod_setor`. Nem todos os setores do shapefile vão ter dados associados: sobram vazios demográficos. Eles desaparecem no merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setores = setores.merge(dados_censo, left_on='CD_GEOCODI', right_on='Cod_setor', how='inner', suffixes=('','__y'))\n",
    "setores = setores[[col for col in setores.columns if '__y' not in col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos, também, colocar ambos no mesmo crs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setores = setores.to_crs(voronois.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos definir uma rotina para aproximar a população de cada voronoi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Estimar população dos voronois\n",
    "Agora já temos todos os elementos necessários para estimar as características da população que vive em cada voronoi. As funções abaixo, posteriormente encapsuladas em uma único, fazem esse cálculo passo a passo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voronois = voronois.reset_index()\n",
    "voronois = voronois.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, cada voronoi deve ser tratado como um objeto geográfico do GeoPandas, para assim permitir o cálculo da interseção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_geodf(row):\n",
    "    # A função pega um GeoSeries e retorna um GeoDataFrame para possibilitar\n",
    "    # operações geográficas no GeoPandas\n",
    "    geo_data = row.geometry\n",
    "    geo_data = gpd.GeoSeries(geo_data)\n",
    "    geo_data = gpd.GeoDataFrame(geo_data)\n",
    "    geo_data.columns = ['geometry']\n",
    "    geo_data.crs = {'init': 'epsg:4326'}\n",
    "    return geo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois, para cada um deses novos GeoDataFrames, selecionamos os setores censitários dentro dele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seleciona_setores(gdf_voronoi, gdf_setores, city_id, whole_country=False):\n",
    "    # Seleciona os setores censitários que estão dentro do voronoi.\n",
    "    # O parâmetro whole_country deve ser true para buscar interseções em todo o país,\n",
    "    # não apenas no contorno do município.\n",
    "    \n",
    "    temp = gdf_setores.copy()\n",
    "    \n",
    "    if whole_country is False:\n",
    "        temp = gdf_setores[gdf_setores.CD_GEOCODM==city_id]\n",
    "        \n",
    "    setores_dentro = gpd.sjoin(temp, gdf_voronoi, how='inner', op='intersects')\n",
    "    return setores_dentro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcula_intersecao(gdf_voronoi, row):\n",
    "    # Calcula a intereseção do setor com o voronoi\n",
    "    \n",
    "    gdf_voronoi.geometry = gdf_voronoi.geometry.buffer(0)\n",
    "    intersection = gdf_voronoi.intersection(row.geometry.buffer(0))\n",
    "    intersection_area = float(intersection.area)\n",
    "    intersection_percentage = float(intersection_area / row.geometry.area)\n",
    "    \n",
    "    return intersection_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcula_valores(voronoi_totals, intersection_percentage, row):\n",
    "    # Usa o valor retornado por calcula_intersecao para adicionar valores\n",
    "    # para cada uma das colunas de dados do voronoi.\n",
    "    # voronoi_totals é modificado inplace, sem necessidade de retornar um valor\n",
    "    \n",
    "    for k,v in voronoi_totals.items(): # voronoi_totals é um elemento criado a partir da variável externa 'keys')\n",
    "        \n",
    "        # Essa condição é para realizar o cálculo de QUANTO DINHEIRO entra, não um percentual da média\n",
    "        # de acordo com a interseção. Trata-se da única variável em que o valor medido não é uma contagem\n",
    "        # de pessoas. Note que, apesar de chamar 'media', nesse momento do fluxo, o valor representa\n",
    "        # uma soma absoluta.\n",
    "        if k == 'media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento':\n",
    "            # IMPLEMENTAR\n",
    "            people = intersection_percentage * row['pessoas_residentes']\n",
    "            to_add = round(people * row[k], 2)\n",
    "        else:\n",
    "            to_add = round(intersection_percentage * row[k], 2) # setor[k] é um campo de dados da linha\n",
    "        \n",
    "        voronoi_totals[k] = voronoi_totals[k] + to_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rodar via df.apply(f, axis=1)\n",
    "def preenche_dados_voronois(row, keys, gdf_setores, whole_country=False):\n",
    "    \n",
    "    # Essa função usa o percentual de interseção e os dados do censo para\n",
    "    # descobrir quantas pessoas de cada característica moram em\n",
    "    \n",
    "    city_id = row.COD_IBGE\n",
    "    voronoi = make_geodf(row)\n",
    "    voronoi_totals = {key: 0 for key in keys}\n",
    "    setores_dentro = seleciona_setores(voronoi, gdf_setores, city_id, whole_country)\n",
    "    \n",
    "    if setores_dentro.shape[0] == 0:\n",
    "        return pd.Series(voronoi_totals)\n",
    "\n",
    "    for index, row_ in setores_dentro.iterrows():\n",
    "        intersection_percentage = calcula_intersecao(voronoi, row_)\n",
    "        calcula_valores(voronoi_totals, intersection_percentage, row_)\n",
    "        \n",
    "    # Calcula uma média de renda\n",
    "    try:\n",
    "        voronoi_totals['media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento'] = voronoi_totals['media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento'] / voronoi_totals['pessoas_residentes']  \n",
    "    except ZeroDivisionError:\n",
    "        voronoi_totals['media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento'] = np.nan\n",
    "        \n",
    "    return pd.Series(voronoi_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcular_percentuais(row, exceptions):\n",
    "    columns = ['eleitores_alfabetizados', 'eleitores_de_16_a_24_anos',\n",
    "   'pessoas_em_idade_eleitoral', 'pessoas_residentes',\n",
    "   'total_alfabetizados', 'total_amarelos', 'total_brancos',\n",
    "   'total_homens', 'total_indigenas', 'total_mulheres',\n",
    "   'total_mulheres_responsaveis_por_domicilio', 'total_pardos',\n",
    "   'total_pretos', 'total_pretos_e_pardos']\n",
    "    \n",
    "    percent_values = { 'pp_' + key:None for key in columns }\n",
    "    \n",
    "    for column in columns:\n",
    "        #print(column)\n",
    "        if column in ['eleitores_alfabetizados', 'eleitores_de_16_a_24_anos']:\n",
    "            col_to_divide = 'pessoas_em_idade_eleitoral'            \n",
    "        else:\n",
    "            col_to_divide = 'pessoas_residentes'\n",
    "\n",
    "        try:\n",
    "            #print(row[column], row[col_to_divide])\n",
    "            percent_values['pp_' + column] = row[column] / row[col_to_divide]\n",
    "        except ZeroDivisionError as e:\n",
    "            #print(\"ZeroDivisionError at index\", row.name)\n",
    "            #print(\"error\")\n",
    "            exceptions.append(row.name)\n",
    "            percent_values['pp_' + column] = 0\n",
    "\n",
    "    return pd.Series(percent_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     21,
     124,
     154
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_voronoi_census_data(setores_arg, voronois_arg):\n",
    "    \n",
    "    # Cópias locais\n",
    "    gdf_setores = setores_arg.copy().reset_index()\n",
    "    if 'media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento' in gdf_setores.columns:\n",
    "        print(\"column is here\")\n",
    "    \n",
    "    gdf_voronois = voronois_arg.copy().reset_index()\n",
    "    print(\"Copy made\")\n",
    "    \n",
    "    # Coloca voronois e setores no mesmo crs\n",
    "    gdf_setores = gdf_setores.to_crs(gdf_voronois.crs)\n",
    "    if gdf_setores.crs == gdf_voronois.crs:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Wrong crs\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Crs converted\")\n",
    "    \n",
    "    # Cria as chaves dos campos calculados\n",
    "    ages = [\n",
    "            'pessoas_com_menos_de_1_ano_de_idade',\n",
    "            'pessoas_de_1_ano_de_idade',\n",
    "            'pessoas_com_2_anos_de_idade',\n",
    "            'pessoas_com_3_anos_de_idade',\n",
    "            'pessoas_com_4_anos_de_idade',\n",
    "            'pessoas_com_5_anos_de_idade',\n",
    "            'pessoas_com_6_anos_de_idade',\n",
    "            'pessoas_com_7_anos_de_idade',\n",
    "            'pessoas_com_8_anos_de_idade',\n",
    "            'pessoas_com_9_anos_de_idade',\n",
    "            'pessoas_com_10_anos_de_idade',\n",
    "            'pessoas_com_11_anos_de_idade',\n",
    "            'pessoas_com_12_anos_de_idade',\n",
    "            'pessoas_com_13_anos_de_idade',\n",
    "            'pessoas_com_14_anos_de_idade',\n",
    "            'pessoas_com_15_anos_de_idade',\n",
    "            'pessoas_com_16_anos_de_idade',\n",
    "            'pessoas_com_17_anos_de_idade',\n",
    "            'pessoas_com_18_anos_de_idade',\n",
    "            'pessoas_com_19_anos_de_idade',\n",
    "            'pessoas_com_20_anos_de_idade',\n",
    "            'pessoas_com_21_anos_de_idade',\n",
    "            'pessoas_com_22_anos_de_idade',\n",
    "            'pessoas_com_23_anos_de_idade',\n",
    "            'pessoas_com_24_anos_de_idade',\n",
    "            'pessoas_com_25_anos_de_idade',\n",
    "            'pessoas_com_26_anos_de_idade',\n",
    "            'pessoas_com_27_anos_de_idade',\n",
    "            'pessoas_com_28_anos_de_idade',\n",
    "            'pessoas_com_29_anos_de_idade',\n",
    "            'pessoas_com_30_anos_de_idade',\n",
    "            'pessoas_com_31_anos_de_idade',\n",
    "            'pessoas_com_32_anos_de_idade',\n",
    "            'pessoas_com_33_anos_de_idade',\n",
    "            'pessoas_com_34_anos_de_idade',\n",
    "            'pessoas_com_35_anos_de_idade',\n",
    "            'pessoas_com_36_anos_de_idade',\n",
    "            'pessoas_com_37_anos_de_idade',\n",
    "            'pessoas_com_38_anos_de_idade',\n",
    "            'pessoas_com_39_anos_de_idade',\n",
    "            'pessoas_com_40_anos_de_idade',\n",
    "            'pessoas_com_41_anos_de_idade',\n",
    "            'pessoas_com_42_anos_de_idade',\n",
    "            'pessoas_com_43_anos_de_idade',\n",
    "            'pessoas_com_44_anos_de_idade',\n",
    "            'pessoas_com_45_anos_de_idade',\n",
    "            'pessoas_com_46_anos_de_idade',\n",
    "            'pessoas_com_47_anos_de_idade',\n",
    "            'pessoas_com_48_anos_de_idade',\n",
    "            'pessoas_com_49_anos_de_idade',\n",
    "            'pessoas_com_50_anos_de_idade',\n",
    "            'pessoas_com_51_anos_de_idade',\n",
    "            'pessoas_com_52_anos_de_idade',\n",
    "            'pessoas_com_53_anos_de_idade',\n",
    "            'pessoas_com_54_anos_de_idade',\n",
    "            'pessoas_com_55_anos_de_idade',\n",
    "            'pessoas_com_56_anos_de_idade',\n",
    "            'pessoas_com_57_anos_de_idade',\n",
    "            'pessoas_com_58_anos_de_idade',\n",
    "            'pessoas_com_59_anos_de_idade',\n",
    "            'pessoas_com_60_anos_de_idade',\n",
    "            'pessoas_com_61_anos_de_idade',\n",
    "            'pessoas_com_62_anos_de_idade',\n",
    "            'pessoas_com_63_anos_de_idade',\n",
    "            'pessoas_com_64_anos_de_idade',\n",
    "            'pessoas_com_65_anos_de_idade',\n",
    "            'pessoas_com_66_anos_de_idade',\n",
    "            'pessoas_com_67_anos_de_idade',\n",
    "            'pessoas_com_68_anos_de_idade',\n",
    "            'pessoas_com_69_anos_de_idade',\n",
    "            'pessoas_com_70_anos_de_idade',\n",
    "            'pessoas_com_71_anos_de_idade',\n",
    "            'pessoas_com_72_anos_de_idade',\n",
    "            'pessoas_com_73_anos_de_idade',\n",
    "            'pessoas_com_74_anos_de_idade',\n",
    "            'pessoas_com_75_anos_de_idade',\n",
    "            'pessoas_com_76_anos_de_idade',\n",
    "            'pessoas_com_77_anos_de_idade',\n",
    "            'pessoas_com_78_anos_de_idade',\n",
    "            'pessoas_com_79_anos_de_idade',\n",
    "            'pessoas_com_80_anos_de_idade',\n",
    "            'pessoas_com_81_anos_de_idade',\n",
    "            'pessoas_com_82_anos_de_idade',\n",
    "            'pessoas_com_83_anos_de_idade',\n",
    "            'pessoas_com_84_anos_de_idade',\n",
    "            'pessoas_com_85_anos_de_idade',\n",
    "            'pessoas_com_86_anos_de_idade',\n",
    "            'pessoas_com_87_anos_de_idade',\n",
    "            'pessoas_com_88_anos_de_idade',\n",
    "            'pessoas_com_89_anos_de_idade',\n",
    "            'pessoas_com_90_anos_de_idade',\n",
    "            'pessoas_com_91_anos_de_idade',\n",
    "            'pessoas_com_92_anos_de_idade',\n",
    "            'pessoas_com_93_anos_de_idade',\n",
    "            'pessoas_com_94_anos_de_idade',\n",
    "            'pessoas_com_95_anos_de_idade',\n",
    "            'pessoas_com_96_anos_de_idade',\n",
    "            'pessoas_com_97_anos_de_idade',\n",
    "            'pessoas_com_98_anos_de_idade',\n",
    "            'pessoas_com_99_anos_de_idade',\n",
    "            'pessoas_com_100_anos_ou_mais_de_idade',\n",
    "        ]\n",
    "    keys = [ \n",
    "     \"pessoas_em_idade_eleitoral\", \"eleitores_de_16_a_24_anos\", \"total_alfabetizados\", \"eleitores_alfabetizados\", \"total_homens\", \"total_mulheres\", \n",
    "     \"total_mulheres_responsaveis_por_domicilio\", \"total_pretos\", \"total_pardos\", \"total_pretos_e_pardos\",\n",
    "     \"total_brancos\", \"total_indigenas\", \"total_amarelos\", \"pessoas_residentes\", \n",
    "     'media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento', # Essa última se tornará uma soma da renda das pessoas que entram no voronoi, assumindo que todas ganham a média\n",
    "    ]\n",
    "    keys.extend(ages)\n",
    "    \n",
    "    # Roda a função que preenche os dados de todas as cidades que existiam no censo 2010\n",
    "    function_return = gdf_voronois.progress_apply(preenche_dados_voronois, axis=1, args=(keys, gdf_setores, False))\n",
    "    gdf_voronois = pd.concat([gdf_voronois, function_return], axis=1).reset_index(drop=True)    \n",
    "    print(\"First batch of data calculated\")\n",
    "    \n",
    "    # Cinco cidades que foram criadas após a realização do Censo não puderam ter os valores preenchidos. \n",
    "    # Agora, realizamos o processo de novo, mas com um sjoin que envolve todo o país. \n",
    "    # Isso é possível porque os setores censitários cobrem todo o território nacional. \n",
    "    # Assim, vamos capturar os dados referentes aos distritos dos quais essas novas cidades se desmembraram.\n",
    "    function_return = (gdf_voronois[gdf_voronois.pessoas_residentes==0]).progress_apply(preenche_dados_voronois, axis=1, args=(keys, gdf_setores, True))\n",
    "    print(\"Second batch of data calculated\")\n",
    "    \n",
    "    # Substitui essasa novas cidades diretamente no índice\n",
    "    gdf_voronois.loc[function_return.index, function_return.columns] = function_return # verificar se erro não pode estar sendo gerado aqui\n",
    "    \n",
    "    # Cria campos que recebem valores percentuais\n",
    "    exceptions = []\n",
    "    function_return = gdf_voronois.progress_apply(calcular_percentuais, axis=1, args=(exceptions,))\n",
    "    gdf_voronois = pd.concat([gdf_voronois, function_return], axis=1).reset_index(drop=True)\n",
    "    print(\"Percentages calculated\")\n",
    "    \n",
    "    # Renomeia colunas\n",
    "    columns = {\n",
    "       'index':'INDEX',\n",
    "       'COD_TSE':'COD_TSE',\n",
    "       'COD_IBGE':'COD_IBGE',\n",
    "       'NM_MUNICIP':'NM_MUNIC',\n",
    "       'UF':'UF',\n",
    "       'ZONA':'ZONA',\n",
    "       'fetched_ad':'FET_AD',\n",
    "       'id_unico':'ID_UNICO',\n",
    "       'lat':'LAT',\n",
    "       'geom_str':'GEOM_STR',\n",
    "       'loc_unico':'LOC_UNICO',\n",
    "       'lon':'LON',\n",
    "       'geometry':'geometry',\n",
    "       'eleitores_alfabetizados':'ELEI_ALF',\n",
    "       'eleitores_de_16_a_24_anos':'ELEI_16_24',\n",
    "       'pessoas_com_100_anos_ou_mais_de_idade':'P100_MAIS',\n",
    "       'pessoas_com_10_anos_de_idade':'P10',\n",
    "       'pessoas_com_11_anos_de_idade':'P11',\n",
    "       'pessoas_com_12_anos_de_idade':'P12',\n",
    "       'pessoas_com_13_anos_de_idade':'P13',\n",
    "       'pessoas_com_14_anos_de_idade':'P14',\n",
    "       'pessoas_com_15_anos_de_idade':'P15',\n",
    "       'pessoas_com_16_anos_de_idade':'P16',\n",
    "       'pessoas_com_17_anos_de_idade':'P17',\n",
    "       'pessoas_com_18_anos_de_idade':'P18',\n",
    "       'pessoas_com_19_anos_de_idade':'P19',\n",
    "       'pessoas_com_20_anos_de_idade':'P20',\n",
    "       'pessoas_com_21_anos_de_idade':'P21',\n",
    "       'pessoas_com_22_anos_de_idade':'P22',\n",
    "       'pessoas_com_23_anos_de_idade':'P23',\n",
    "       'pessoas_com_24_anos_de_idade':'P24',\n",
    "       'pessoas_com_25_anos_de_idade':'P25',\n",
    "       'pessoas_com_26_anos_de_idade':'P26',\n",
    "       'pessoas_com_27_anos_de_idade':'P27',\n",
    "       'pessoas_com_28_anos_de_idade':'P28',\n",
    "       'pessoas_com_29_anos_de_idade':'P29',\n",
    "       'pessoas_com_2_anos_de_idade':'P2',\n",
    "       'pessoas_com_30_anos_de_idade':'P30',\n",
    "       'pessoas_com_31_anos_de_idade':'P31',\n",
    "       'pessoas_com_32_anos_de_idade':'P32',\n",
    "       'pessoas_com_33_anos_de_idade':'P33',\n",
    "       'pessoas_com_34_anos_de_idade':'P34',\n",
    "       'pessoas_com_35_anos_de_idade':'P35',\n",
    "       'pessoas_com_36_anos_de_idade':'P36',\n",
    "       'pessoas_com_37_anos_de_idade':'P37',\n",
    "       'pessoas_com_38_anos_de_idade':'P38',\n",
    "       'pessoas_com_39_anos_de_idade':'P39',\n",
    "       'pessoas_com_3_anos_de_idade':'P3',\n",
    "       'pessoas_com_40_anos_de_idade':'P40',\n",
    "       'pessoas_com_41_anos_de_idade':'P41',\n",
    "       'pessoas_com_42_anos_de_idade':'P42',\n",
    "       'pessoas_com_43_anos_de_idade':'P43',\n",
    "       'pessoas_com_44_anos_de_idade':'P44',\n",
    "       'pessoas_com_45_anos_de_idade':'P45',\n",
    "       'pessoas_com_46_anos_de_idade':'P46',\n",
    "       'pessoas_com_47_anos_de_idade':'P47',\n",
    "       'pessoas_com_48_anos_de_idade':'P48',\n",
    "       'pessoas_com_49_anos_de_idade':'P49',\n",
    "       'pessoas_com_4_anos_de_idade':'P4',\n",
    "       'pessoas_com_50_anos_de_idade':'P50',\n",
    "       'pessoas_com_51_anos_de_idade':'P51',\n",
    "       'pessoas_com_52_anos_de_idade':'P52',\n",
    "       'pessoas_com_53_anos_de_idade':'P53',\n",
    "       'pessoas_com_54_anos_de_idade':'P54',\n",
    "       'pessoas_com_55_anos_de_idade':'P55',\n",
    "       'pessoas_com_56_anos_de_idade':'P56',\n",
    "       'pessoas_com_57_anos_de_idade':'P57',\n",
    "       'pessoas_com_58_anos_de_idade':'P58',\n",
    "       'pessoas_com_59_anos_de_idade':'P59',\n",
    "       'pessoas_com_5_anos_de_idade':'P5',\n",
    "       'pessoas_com_60_anos_de_idade':'P60',\n",
    "       'pessoas_com_61_anos_de_idade':'P61',\n",
    "       'pessoas_com_62_anos_de_idade':'P62',\n",
    "       'pessoas_com_63_anos_de_idade':'P63',\n",
    "       'pessoas_com_64_anos_de_idade':'P64',\n",
    "       'pessoas_com_65_anos_de_idade':'P65',\n",
    "       'pessoas_com_66_anos_de_idade':'P66',\n",
    "       'pessoas_com_67_anos_de_idade':'P67',\n",
    "       'pessoas_com_68_anos_de_idade':'P68',\n",
    "       'pessoas_com_69_anos_de_idade':'P69',\n",
    "       'pessoas_com_6_anos_de_idade':'P6',\n",
    "       'pessoas_com_70_anos_de_idade':'P70',\n",
    "       'pessoas_com_71_anos_de_idade':'P71',\n",
    "       'pessoas_com_72_anos_de_idade':'P72',\n",
    "       'pessoas_com_73_anos_de_idade':'P73',\n",
    "       'pessoas_com_74_anos_de_idade':'P74',\n",
    "       'pessoas_com_75_anos_de_idade':'P75',\n",
    "       'pessoas_com_76_anos_de_idade':'P76',\n",
    "       'pessoas_com_77_anos_de_idade':'P77',\n",
    "       'pessoas_com_78_anos_de_idade':'P78',\n",
    "       'pessoas_com_79_anos_de_idade':'P79',\n",
    "       'pessoas_com_7_anos_de_idade':'P7',\n",
    "       'pessoas_com_80_anos_de_idade':'P80',\n",
    "       'pessoas_com_81_anos_de_idade':'P81',\n",
    "       'pessoas_com_82_anos_de_idade':'P82',\n",
    "       'pessoas_com_83_anos_de_idade':'P83',\n",
    "       'pessoas_com_84_anos_de_idade':'P84',\n",
    "       'pessoas_com_85_anos_de_idade':'P85',\n",
    "       'pessoas_com_86_anos_de_idade':'P86',\n",
    "       'pessoas_com_87_anos_de_idade':'P87',\n",
    "       'pessoas_com_88_anos_de_idade':'P88',\n",
    "       'pessoas_com_89_anos_de_idade':'P89',\n",
    "       'pessoas_com_8_anos_de_idade':'P8',\n",
    "       'pessoas_com_90_anos_de_idade':'P90',\n",
    "       'pessoas_com_91_anos_de_idade':'P91',\n",
    "       'pessoas_com_92_anos_de_idade':'P92',\n",
    "       'pessoas_com_93_anos_de_idade':'P93',\n",
    "       'pessoas_com_94_anos_de_idade':'P94',\n",
    "       'pessoas_com_95_anos_de_idade':'P95',\n",
    "       'pessoas_com_96_anos_de_idade':'P96',\n",
    "       'pessoas_com_97_anos_de_idade':'P97',\n",
    "       'pessoas_com_98_anos_de_idade':'P98',\n",
    "       'pessoas_com_99_anos_de_idade':'P99',\n",
    "       'pessoas_com_9_anos_de_idade':'P9',\n",
    "       'pessoas_com_menos_de_1_ano_de_idade':'P1_MENOS',\n",
    "       'pessoas_de_1_ano_de_idade':'P1',\n",
    "       'pessoas_em_idade_eleitoral':'P_ELEI',\n",
    "       'pessoas_residentes':'P_RESID',\n",
    "       'media_renda_nom_mensal_pessoas_com_10_anos_ou_mais_com_ou_sem_rendimento':'RENDA_MEDI',\n",
    "       'total_alfabetizados':'TOTAL_ALF',\n",
    "       'total_amarelos':'TT_AMARELO',\n",
    "       'total_brancos':'TT_BRANCO',\n",
    "       'total_homens':'TT_HOMEM',\n",
    "       'total_indigenas':'TT_INDIO',\n",
    "       'total_mulheres':'TT_MULH',\n",
    "       'total_mulheres_responsaveis_por_domicilio':'TT_MULH_DO',\n",
    "       'total_pardos':'TT_PARDO',\n",
    "       'total_pretos':'TT_PRETO',\n",
    "       'total_pretos_e_pardos':'TT_PRE_PAR',\n",
    "       'pp_eleitores_alfabetizados':'PP_ELEIALF',\n",
    "       'pp_eleitores_de_16_a_24_anos':'PP_ELE1624',\n",
    "       'pp_pessoas_em_idade_eleitoral':'PP_IDD_ELE',\n",
    "       'pp_pessoas_residentes':'PP_RESIDE',\n",
    "       'pp_total_alfabetizados':'PP_TT_ALF',\n",
    "       'pp_total_amarelos':'PP_TT_AMAR',\n",
    "       'pp_total_brancos':'PP_TT_BRAN',\n",
    "       'pp_total_homens':'PP_TT_HMM',\n",
    "       'pp_total_indigenas':'PP_TT_INDI',\n",
    "       'pp_total_mulheres':'PP_TT_MULH',\n",
    "       'pp_total_mulheres_responsaveis_por_domicilio':'PP_MULH_DO',\n",
    "       'pp_total_pardos':'PP_TT_PARD',\n",
    "       'pp_total_pretos':'PP_TT_PRET',\n",
    "       'pp_total_pretos_e_pardos':'PP_T_PREPA',\n",
    "     }\n",
    "    \n",
    "    gdf_voronois = gdf_voronois.rename(columns=columns)\n",
    "    to_keep = [col for col in gdf_voronois.columns if col in columns.values()]\n",
    "    party_codes = [col for col in gdf_voronois.columns if col.isdigit()]\n",
    "    to_keep.extend(party_codes)\n",
    "    to_keep.extend([party_code + \"_PP\" for party_code in party_codes])\n",
    "    gdf_voronois = gdf_voronois[to_keep]\n",
    "    \n",
    "    # Agora esse pedaço de código vai me fazer parecer um imbecil.\n",
    "    # O geopandas se recusa a salvar o shapefile com todas as colunas de dados.\n",
    "    # Entretanto, se eu salvar os polígonos em um shapefile, os dados em um csv\n",
    "    # e depois lê-los do disco e fazer um merge em campo comum, eu consigo salvar tudo\n",
    "    # em um mesmo shapefile. Vá entender!\n",
    "    try:\n",
    "        fp_a = \"../data/geo/voronois-finalizados/polygons-\" + str(datetime.now())[:19] + \".shp\"\n",
    "        fp_b = \"../data/geo/voronois-finalizados/data-\" + str(datetime.now())[:19] + \".csv\"\n",
    "        fp_c = \"../data/geo/voronois-finalizados/polygons-with-data-\" + str(datetime.now())[:19] + \".shp\"\n",
    "        \n",
    "        gdf_voronois[['geometry','ID_UNICO']].to_file(fp_a)  \n",
    "        gdf_voronois[[col for col in gdf_voronois.columns if col != 'geometry']].to_csv(fp_b, index=False)\n",
    "        \n",
    "        print(\"Files saved\")\n",
    "        \n",
    "        # Re-lê os dados, faz merge e salva em um arquivo só\n",
    "        a = gpd.read_file(fp_a)\n",
    "        b = pd.read_csv(fp_b, dtype={\"ID_UNICO\":\"str\"})\n",
    "        c = a.merge(b, on='ID_UNICO')        \n",
    "        c.to_file(fp_c)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Exception happened when saving\")\n",
    "        print(e)\n",
    "    \n",
    "    return gdf_voronois, exceptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voros_, exceptions = estimate_voronoi_census_data(setores, voronois)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geopandas]",
   "language": "python",
   "name": "conda-env-geopandas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 437.666666,
   "position": {
    "height": "40px",
    "left": "759.667px",
    "right": "20px",
    "top": "48px",
    "width": "609px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
